{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expanded-houston",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n8-3: implemented exclusion criteria based on percent TRs and whether or not there was no bpress\\n8-16: destroyed everything that wasn't necessary for creating current head motion criteria\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#python resample.py $IMAGE_TO_RESAMPLE $REFERENCE_IMAGE\n",
    "\"\"\"\n",
    "8-3: implemented exclusion criteria based on percent TRs and whether or not there was no bpress\n",
    "8-16: destroyed everything that wasn't necessary for creating current head motion criteria\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spanish-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter nbconvert --to python slurm_create-data_preproc.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "australian-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "from nilearn.input_data import NiftiMasker , MultiNiftiMasker\n",
    "import nilearn as nil\n",
    "import numpy as np \n",
    "import os\n",
    "import os.path\n",
    "import scipy.io\n",
    "import nibabel as nib\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.masking import compute_epi_mask\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "import sys  \n",
    "import random\n",
    "# import logging\n",
    "\n",
    "import deepdish as dd\n",
    "import numpy as np\n",
    "\n",
    "import brainiak.eventseg.event\n",
    "import nibabel as nib\n",
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, zscore, pearsonr\n",
    "from scipy.signal import gaussian, convolve\n",
    "from sklearn import decomposition\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns \n",
    "\n",
    "#%autosave 5\n",
    "#%matplotlib inline\n",
    "sns.set(style = 'white', context='talk', font_scale=1, rc={\"lines.linewidth\": 2})\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "random.seed(10)\n",
    "#%matplotlib inline\n",
    "from brainiak import image, io\n",
    "from scipy.stats import stats\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from brainiak import image, io\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "import pandas as pd\n",
    "# Import machine learning libraries\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold, f_classif, SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import sem\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import statistics\n",
    "# Visualize it as an ROI\n",
    "from nilearn.plotting import plot_roi\n",
    "#plot_roi(x)\n",
    "from nilearn.image import concat_imgs, resample_img, mean_img\n",
    "from nilearn.plotting import view_img\n",
    "from nilearn import datasets, plotting\n",
    "from nilearn.input_data import NiftiSpheresMasker\n",
    "\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "from nilearn.image import concat_imgs, resample_img, mean_img,index_img\n",
    "from nilearn import image\n",
    "from nilearn import masking\n",
    "from nilearn.plotting import view_img\n",
    "from nilearn.image import resample_to_img\n",
    "import math\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-tension",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "alpha-maryland",
   "metadata": {},
   "source": [
    "# Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-hotel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "institutional-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epi_data(sub, ses, task,run, space):\n",
    "  # Load MRI file\n",
    "    if space == \"MNI\":\n",
    "        epi_in = os.path.join(data_dir, sub, ses, 'func', \"%s_%s_task-%s_run-%s_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\" % (sub, ses, task,run))\n",
    "    elif space == \"T1\":\n",
    "        epi_in = os.path.join(data_dir, sub, ses, 'func', \"%s_%s_task-%s_run-%s_space-T1w_desc-preproc_bold.nii.gz\" % (sub, ses, task,run))\n",
    "    else:\n",
    "        print(\"wrong load epi input. check this function\")\n",
    "    epi_data = nib.load(epi_in)\n",
    "    print(epi_data.shape)\n",
    "    print(\"Loading data from %s\" % (epi_in))\n",
    "    return epi_data\n",
    "\n",
    "def load_roi_mask(ROI_name, space):\n",
    "    if space == \"MNI\":\n",
    "        maskdir = os.path.join(rois_dir)    \n",
    "        print(\"expected shape: 78, 93,65\")\n",
    "    elif space == \"T1\":\n",
    "        maskdir = os.path.join(rois_dir+ \"/T1\")\n",
    "        print(\"expected shape: 56, 72,53\")\n",
    "    else:\n",
    "        print(\"wrong mask input. check this function\")\n",
    "    # load the mask\n",
    "    maskfile = os.path.join(maskdir, \"%s.nii\" % (ROI_name))\n",
    "    mask = nib.load(maskfile)\n",
    "    print(\"mask shape: \", mask.shape)\n",
    "    print(\"Loaded %s mask\" % (ROI_name))\n",
    "    return mask\n",
    "def intersect_mask(sub, num_runs,reg, ses=\"ses-01\",task=\"Attn\"):\n",
    "    # This is based off of 'load_data' function in template\n",
    "    # Loads all fMRI runs into a matrix #\n",
    "    \"\"\"\n",
    "    reg = T1 or MNI registration?\n",
    "    norm_type = by Space or by Time? \n",
    "    \"\"\"\n",
    "    yoz = []\n",
    "    print(\"Begin intersecting, you sexy beast\")\n",
    "    for run in range(1, num_runs + 1):\n",
    "        if sub == \"sub-002\":\n",
    "            if run >=7:\n",
    "                run = run+1\n",
    "        # Load epi data \n",
    "        epi = load_epi_data(sub,ses,task,run,reg)\n",
    "        # Mask data\n",
    "        roi_samp = compute_epi_mask(epi) # -- whole brain\n",
    "        #roi_samp load_roi_mask(ROI_name,reg) # -- mask\n",
    "\n",
    "        nifti_masker = NiftiMasker(mask_img=roi_samp)\n",
    "        maskedData = nifti_masker.fit_transform(epi)\n",
    "        yoz.append(roi_samp)\n",
    "    #print(concatenated_data)\n",
    "    epi_data = nil.masking.intersect_masks(yoz)\n",
    "    print(\"all done wit da intersextion (lol)\")\n",
    "\n",
    "    return epi_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def label_lists(label, num_tr):\n",
    "    b = [[]]\n",
    "    a = []\n",
    "    for i in label:\n",
    "        # substring label in psychopy output\n",
    "        # if the first three characters == M_s, etc, then add correct indext to string\n",
    "        if i[1:4] == \"M_s\":\n",
    "            a.append(\"SM\")\n",
    "            b.append([0]*num_tr)\n",
    "        elif i[1:4] == \"C_s\":\n",
    "            a.append(\"SC\")\n",
    "            b.append([1]*num_tr)        \n",
    "        elif i[1:4] == \"M_o\":\n",
    "            a.append(\"OM\")\n",
    "            b.append([2]*num_tr)\n",
    "        elif i[1:4] == \"C_o\":\n",
    "            a.append(\"OC\")\n",
    "            b.append([3]*num_tr)     \n",
    "        else:\n",
    "            a.append(\"Re\")\n",
    "            b.append([4]*num_tr)     \n",
    "    return a, b[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-branch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cosmetic-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fMRI3d(sub, num_runs,reg, norm_type, ses=\"ses-01\",task=\"Attn\"):\n",
    "    # This is based off of 'load_data' function in template\n",
    "    # Loads all fMRI runs into a matrix #\n",
    "    \"\"\"\n",
    "    reg = T1 or MNI registration?\n",
    "    norm_type = by Space or by Time? - default is by space (rows)\n",
    "    \"\"\"\n",
    "    concatenated_data = []\n",
    "    int_mask = intersect_mask(sub, 1,reg)\n",
    "    # \n",
    "    for run in range(1, num_runs + 1):\n",
    "        if sub == \"sub-002\":\n",
    "            if run >=7:\n",
    "                run = run+1\n",
    "        if sub != \"sub-010\":\n",
    "            # Load epi data \n",
    "            epi = load_epi_data(sub,ses,task,run,reg)\n",
    "        else:\n",
    "            # Load epi data \n",
    "            print(\"sub-10, watch out\")\n",
    "            bad_epi = load_epi_data(sub,ses,task,run,reg)\n",
    "            good_epi = load_epi_data(\"sub-001\",ses,task,run,reg)\n",
    "            epi = resample_to_img(bad_epi , good_epi, interpolation='nearest')\n",
    "            int_mask = resample_to_img(int_mask , good_epi, interpolation='nearest')\n",
    "        # delete first 9 TRs\n",
    "        epi = index_img(epi,slice(9,209))\n",
    "        \n",
    "        # load confounds\n",
    "        run_conf = np.asarray(pd.read_csv(os.path.join(confounds + sub + \"/func/\", \n",
    "                                                           '%s_ses-01_task-Attn_run-%s_desc-model_timeseries.csv') % (sub, run)))\n",
    "        print(run_conf.shape)\n",
    "        # clean image\n",
    "        # low_pass= .1,\n",
    "        clean_bold = image.clean_img(epi, standardize = False, confounds = run_conf[9:], high_pass=1/128, \n",
    "                                   t_r=1.5, mask_img = int_mask)\n",
    "        \n",
    "        \n",
    "        #Smooth\n",
    "        clean_bold = image.smooth_img(clean_bold, fwhm=5)\n",
    "        \n",
    "        # F*k it mask off -- Load ROI data\n",
    "        #roi_samp =load_roi_mask(ROI_name,reg)\n",
    "        # Pull voxels from epi data # *** may need to change this \n",
    "        #nifti_masker = NiftiMasker(mask_img=roi_samp)\n",
    "        #masked_data = nifti_masker.fit_transform(clean_bold)\n",
    "        \n",
    "        #append to cat_dat\n",
    "        concatenated_data.append(clean_bold)\n",
    "    \"FINISHED YAY BEAST\"\n",
    "    return concatenated_data, epi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "affiliated-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fMRI3d(sub, num_runs,reg, norm_type, mask, ses=\"ses-01\",task=\"Attn\"):\n",
    "    # This is based off of 'load_data' function in template\n",
    "    # Loads all fMRI runs into a matrix #\n",
    "    \"\"\"\n",
    "    reg = T1 or MNI registration?\n",
    "    norm_type = by Space or by Time? - default is by space (rows)\n",
    "    \"\"\"\n",
    "    concatenated_data = []\n",
    "    \n",
    "    for run in range(1, num_runs + 1):\n",
    "        if sub == \"sub-002\":\n",
    "            if run >=7:\n",
    "                run = run+1\n",
    "        if sub != \"sub-010\":\n",
    "            # Load epi data \n",
    "            epi = load_epi_data(sub,ses,task,run,reg)\n",
    "        else:\n",
    "            # Load epi data \n",
    "            print(\"sub-10, watch out\")\n",
    "            bad_epi = load_epi_data(sub,ses,task,run,reg)\n",
    "            good_epi = load_epi_data(\"sub-001\",ses,task,run,reg)\n",
    "            epi = resample_to_img(bad_epi , good_epi, interpolation='nearest')\n",
    "        # delete first 9 TRs\n",
    "        epi = index_img(epi,slice(9,209))\n",
    "        \n",
    "        # load confounds\n",
    "        run_conf = np.asarray(pd.read_csv(os.path.join(confounds + sub + \"/func/\", \n",
    "                                                           '%s_ses-01_task-Attn_run-%s_desc-model_timeseries.csv') % (sub, run)))\n",
    "        print(run_conf.shape)\n",
    "        # clean image\n",
    "        # low_pass= .1,\n",
    "        clean_bold = image.clean_img(epi, standardize = False, confounds = run_conf[9:], high_pass=1/128, \n",
    "                                   t_r=1.5, mask_img = mask)\n",
    "        \n",
    "        \n",
    "        #Smooth\n",
    "        clean_bold = image.smooth_img(clean_bold, fwhm=5)\n",
    "        \n",
    "        # F*k it mask off -- Load ROI data\n",
    "        #roi_samp =load_roi_mask(ROI_name,reg)\n",
    "        # Pull voxels from epi data # *** may need to change this \n",
    "        #nifti_masker = NiftiMasker(mask_img=roi_samp)\n",
    "        #masked_data = nifti_masker.fit_transform(clean_bold)\n",
    "        \n",
    "        #append to cat_dat\n",
    "        concatenated_data.append(clean_bold)\n",
    "    \"FINISHED YAY BEAST\"\n",
    "    return concatenated_data, epi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "corrected-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_indices(behav_p, sub):\n",
    "    behav = pd.read_csv(os.path.join(behav_p, '%s_behav_cleaned.csv') % (sub))\n",
    "    # Define the column in behav to be used for creating labels # \n",
    "    label = behav.iloc[:,1]\n",
    "    # Create an array of labels [1] AND the order in which runs occured [0]#\n",
    "    sub_ses_labels = label_lists(label, 200)\n",
    "    ## Find run sequence, extraction condition indexes from behav data ## \n",
    "    return find_cond_index(sub_ses_labels[0])\n",
    "\n",
    "def find_cond_index(sub_ses_labels):\n",
    "    \"\"\"\n",
    "    For the array of ordered run names (i.e.'Re', 'SM',) find the two indexes per condition\n",
    "    \"\"\" \n",
    "    lab_inx = []\n",
    "\n",
    "    a = []\n",
    "    b = []\n",
    "    c = []\n",
    "    d = []\n",
    "\n",
    "    for i in enumerate(sub_ses_labels):\n",
    "        if i[1] == \"SM\":\n",
    "            # append the index according to where it appeared in the array\n",
    "            a.append(i[0])\n",
    "        if i[1] == \"SC\":\n",
    "            b.append(i[0])\n",
    "        if i[1] == \"OM\":\n",
    "            c.append(i[0])\n",
    "        if i[1] == \"OC\":\n",
    "            d.append(i[0])\n",
    "\n",
    "    # Create a dictionary where each key contains the appropriate indexes\n",
    "    lab_indic = {\n",
    "        'SM' : a,\n",
    "        'SC' : b,\n",
    "        'OM' : c,\n",
    "        'OC' : d,\n",
    "        'RE' : [0,9]\n",
    "    }\n",
    "    return lab_indic \n",
    "    #np.vstack(lab_inx, [\"SM\", \"SC\", \"OM\", \"OC\"])\n",
    "    \n",
    "\n",
    "def load_confounds(cond_list, sub_list,behav_p,confounds):\n",
    "    \"\"\"\n",
    "    args: \n",
    "        cond_list: list of conditions (cond_list=np.array(['SM','SC']))\n",
    "        sub_list: subjects to extract confounds for\n",
    "        behav_p: path to the behavioral data\n",
    "        confounds: path to the confound data\n",
    "    returns:\n",
    "        nested dictionary in the form of: conf_sub[sub][cond][img_ind]\n",
    "        where img_index is the first or second run\n",
    "    \"\"\"\n",
    "    # Confound files\n",
    "\n",
    "    conf_sub = {}\n",
    "    for sub in sub_list:\n",
    "        conf_cond = {}\n",
    "        for cond in cond_list:\n",
    "            confs = []\n",
    "            lab_indic = fnd_indices(sub, behav_p)\n",
    "            ## plus one because indices are 0 thru 9 ## \n",
    "            run_1_ind = lab_indic[cond][0] +1\n",
    "            run_2_ind = lab_indic[cond][1] +1\n",
    "            if sub == 'sub-002':\n",
    "                if run_1_ind >=7 : run_1_ind +=1\n",
    "                if run_2_ind >=7 : run_2_ind +=1\n",
    "            \n",
    "            # Confound load #    \n",
    "            confs.append(np.asarray(pd.read_csv(os.path.join(confounds + sub + \"/func/\", \n",
    "                                                             '%s_ses-01_task-Attn_run-%s_desc-model_timeseries.csv') % (sub, run_1_ind)))[4:,:])\n",
    "            confs.append(np.asarray(pd.read_csv(os.path.join(confounds + sub + \"/func/\",\n",
    "                                                             '%s_ses-01_task-Attn_run-%s_desc-model_timeseries.csv') % (sub, run_2_ind)))[4:,:])\n",
    "            conf_cond[cond] = confs\n",
    "        conf_sub[sub] = conf_cond\n",
    "    return conf_sub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sharp-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnd_indices(sub,behav_p):\n",
    "    behav = pd.read_csv(os.path.join(behav_p, '%s_behav_cleaned.csv') % (sub))\n",
    "    # Define the column in behav to be used for creating labels # \n",
    "    label = behav.iloc[:,1]\n",
    "    # Create an array of labels [1] AND the order in which runs occured [0]#\n",
    "    sub_ses_labels = label_lists(label, 200)\n",
    "    ## Find run sequence, extraction condition indexes from behav data ## \n",
    "    return find_cond_index(sub_ses_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-least",
   "metadata": {},
   "source": [
    "# Define Static VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "confirmed-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "data_dir = \"/jukebox/graziano/coolCatIsaac/ATM/data/bids/derivatives/fmriprep/\"\n",
    "rois_dir = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/rois/\"\n",
    "behav_p = '/jukebox/graziano/coolCatIsaac/ATM/data/behavioral'\n",
    "sav_work = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/results/corr_data/\"\n",
    "load_work = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/results/corr_data/\"\n",
    "confounds = '/jukebox/graziano/coolCatIsaac/ATM/data/bids/derivatives/fmriprep/afni-head_mot/'\n",
    "workspace = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/\"\n",
    "parc_dir = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/rois/schaef_par/MNI/\"\n",
    "sav_fcma = '/jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/load_fcma'\n",
    "censor_dir = '/jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/censor_hm'\n",
    "utils = '/jukebox/graziano/coolCatIsaac/ATM/code/analysis/bpress/utils.py'\n",
    "load_bpress = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/results/bpress_GLM/behav\"\n",
    "confounds_dir = '/jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/censor_hm/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "prerequisite-trial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/censor_hm'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "censor_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "laden-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import fnd_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-providence",
   "metadata": {},
   "source": [
    "# lesss do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "smart-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GENERAL ####\n",
    "sub_list = [\"sub-000\",\"sub-001\",\"sub-003\",\"sub-004\",\"sub-005\",\"sub-006\",\"sub-007\",\"sub-008\",\"sub-009\",\n",
    "            \"sub-011\",\"sub-012\",\"sub-013\",\"sub-014\"]\n",
    "sub_list = [\"sub-000\",\"sub-001\",\"sub-005\",\"sub-006\",\"sub-007\",\"sub-008\",\"sub-009\",\n",
    "            \"sub-011\",\"sub-012\",\"sub-013\",\"sub-014\"]\n",
    "sub_list = [\"sub-015\", \"sub-016\",\"sub-017\", \"sub-018\", \"sub-019\", \"sub-020\",\"sub-021\", \"sub-021\"]\n",
    "sub_list = [\"sub-000\",\"sub-001\",\"sub-002\",\"sub-003\",\"sub-004\",\"sub-005\",\"sub-006\",\"sub-007\",\"sub-008\",\"sub-009\",\n",
    "            \"sub-010\",\"sub-011\",\"sub-012\",\"sub-013\",\"sub-014\",\"sub-015\", \"sub-016\",\"sub-017\", \n",
    "            \"sub-018\", \"sub-019\", \"sub-020\",\"sub-021\"]\n",
    "\"\"\"\n",
    "sub_list = [\"sub-011\",\"sub-012\",\"sub-013\",\"sub-014\",\"sub-015\", \"sub-016\",\"sub-017\", \n",
    "            \"sub-018\", \"sub-019\", \"sub-020\",\"sub-021\"]\n",
    "            \n",
    "\"\"\"\n",
    "sub_list = [\"sub-000\",\"sub-001\",\"sub-002\",\"sub-003\",\"sub-004\",\"sub-005\",\"sub-006\",\"sub-007\",\"sub-008\",\"sub-009\",\n",
    "            \"sub-010\",\"sub-011\",\"sub-012\",\"sub-013\",\"sub-014\",\"sub-015\", \"sub-016\",\"sub-017\", \n",
    "            \"sub-018\", \"sub-019\", \"sub-020\",\"sub-021\",'sub-022','sub-023','sub-024','sub-025','sub-026','sub-027']\n",
    "#sub_list = [\"sub-005\"]\n",
    "\n",
    "# censor threshold\n",
    "num_runs = 10\n",
    "### Change me ## \n",
    "fd = .3\n",
    "thresh= str(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-cornell",
   "metadata": {},
   "source": [
    "# Find the censor matrix of ones for each censored TR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "logical-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_censTR_mat(cens_tr, tr_len=209):\n",
    "    \"\"\"\n",
    "    args: \n",
    "        cens_tr: indices of censored TRs\n",
    "    \"\"\"\n",
    "    if cens_tr.shape[0] == 0:\n",
    "        return cens_tr\n",
    "    else:\n",
    "        cens_mat = np.zeros((tr_len, int(len(cens_tr))))\n",
    "        for idx, tr in enumerate(cens_tr):\n",
    "            cens_col = np.zeros((tr_len))\n",
    "            cens_col[tr] = 1\n",
    "            cens_mat[:,idx] = cens_col\n",
    "        print(\"censoring \", cens_mat.shape[1], 'trs')\n",
    "    return cens_mat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "covered-amendment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "censoring  27 trs\n",
      "36\n",
      "censoring  36 trs\n",
      "45\n",
      "censoring  45 trs\n",
      "23\n",
      "censoring  23 trs\n",
      "5\n",
      "censoring  5 trs\n",
      "9\n",
      "censoring  9 trs\n",
      "34\n",
      "censoring  34 trs\n",
      "43\n",
      "censoring  43 trs\n",
      "16\n",
      "censoring  16 trs\n",
      "12\n",
      "censoring  12 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "9\n",
      "censoring  9 trs\n",
      "10\n",
      "censoring  10 trs\n",
      "31\n",
      "censoring  31 trs\n",
      "23\n",
      "censoring  23 trs\n",
      "9\n",
      "censoring  9 trs\n",
      "34\n",
      "censoring  34 trs\n",
      "40\n",
      "censoring  40 trs\n",
      "29\n",
      "censoring  29 trs\n",
      "12\n",
      "censoring  12 trs\n",
      "9\n",
      "censoring  9 trs\n",
      "61\n",
      "censoring  61 trs\n",
      "32\n",
      "censoring  32 trs\n",
      "19\n",
      "censoring  19 trs\n",
      "7\n",
      "censoring  7 trs\n",
      "36\n",
      "censoring  36 trs\n",
      "12\n",
      "censoring  12 trs\n",
      "14\n",
      "censoring  14 trs\n",
      "82\n",
      "censoring  82 trs\n",
      "11\n",
      "censoring  11 trs\n",
      "8\n",
      "censoring  8 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "32\n",
      "censoring  32 trs\n",
      "34\n",
      "censoring  34 trs\n",
      "13\n",
      "censoring  13 trs\n",
      "30\n",
      "censoring  30 trs\n",
      "31\n",
      "censoring  31 trs\n",
      "46\n",
      "censoring  46 trs\n",
      "33\n",
      "censoring  33 trs\n",
      "49\n",
      "censoring  49 trs\n",
      "5\n",
      "censoring  5 trs\n",
      "0\n",
      "10\n",
      "censoring  10 trs\n",
      "14\n",
      "censoring  14 trs\n",
      "19\n",
      "censoring  19 trs\n",
      "17\n",
      "censoring  17 trs\n",
      "23\n",
      "censoring  23 trs\n",
      "8\n",
      "censoring  8 trs\n",
      "18\n",
      "censoring  18 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "0\n",
      "4\n",
      "censoring  4 trs\n",
      "80\n",
      "censoring  80 trs\n",
      "38\n",
      "censoring  38 trs\n",
      "92\n",
      "censoring  92 trs\n",
      "9\n",
      "censoring  9 trs\n",
      "56\n",
      "censoring  56 trs\n",
      "134\n",
      "censoring  134 trs\n",
      "116\n",
      "censoring  116 trs\n",
      "8\n",
      "censoring  8 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "10\n",
      "censoring  10 trs\n",
      "66\n",
      "censoring  66 trs\n",
      "56\n",
      "censoring  56 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "51\n",
      "censoring  51 trs\n",
      "77\n",
      "censoring  77 trs\n",
      "18\n",
      "censoring  18 trs\n",
      "14\n",
      "censoring  14 trs\n",
      "0\n",
      "19\n",
      "censoring  19 trs\n",
      "17\n",
      "censoring  17 trs\n",
      "38\n",
      "censoring  38 trs\n",
      "20\n",
      "censoring  20 trs\n",
      "89\n",
      "censoring  89 trs\n",
      "85\n",
      "censoring  85 trs\n",
      "19\n",
      "censoring  19 trs\n",
      "86\n",
      "censoring  86 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "0\n",
      "1\n",
      "censoring  1 trs\n",
      "47\n",
      "censoring  47 trs\n",
      "7\n",
      "censoring  7 trs\n",
      "91\n",
      "censoring  91 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "68\n",
      "censoring  68 trs\n",
      "69\n",
      "censoring  69 trs\n",
      "15\n",
      "censoring  15 trs\n",
      "0\n",
      "2\n",
      "censoring  2 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "7\n",
      "censoring  7 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "14\n",
      "censoring  14 trs\n",
      "5\n",
      "censoring  5 trs\n",
      "0\n",
      "2\n",
      "censoring  2 trs\n",
      "4\n",
      "censoring  4 trs\n",
      "12\n",
      "censoring  12 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "56\n",
      "censoring  56 trs\n",
      "28\n",
      "censoring  28 trs\n",
      "8\n",
      "censoring  8 trs\n",
      "95\n",
      "censoring  95 trs\n",
      "41\n",
      "censoring  41 trs\n",
      "32\n",
      "censoring  32 trs\n",
      "63\n",
      "censoring  63 trs\n",
      "10\n",
      "censoring  10 trs\n",
      "7\n",
      "censoring  7 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "56\n",
      "censoring  56 trs\n",
      "67\n",
      "censoring  67 trs\n",
      "28\n",
      "censoring  28 trs\n",
      "34\n",
      "censoring  34 trs\n",
      "33\n",
      "censoring  33 trs\n",
      "29\n",
      "censoring  29 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "60\n",
      "censoring  60 trs\n",
      "59\n",
      "censoring  59 trs\n",
      "57\n",
      "censoring  57 trs\n",
      "42\n",
      "censoring  42 trs\n",
      "69\n",
      "censoring  69 trs\n",
      "46\n",
      "censoring  46 trs\n",
      "77\n",
      "censoring  77 trs\n",
      "80\n",
      "censoring  80 trs\n",
      "73\n",
      "censoring  73 trs\n",
      "64\n",
      "censoring  64 trs\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "censoring  3 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "10\n",
      "censoring  10 trs\n",
      "4\n",
      "censoring  4 trs\n",
      "0\n",
      "2\n",
      "censoring  2 trs\n",
      "5\n",
      "censoring  5 trs\n",
      "8\n",
      "censoring  8 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "42\n",
      "censoring  42 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "7\n",
      "censoring  7 trs\n",
      "13\n",
      "censoring  13 trs\n",
      "8\n",
      "censoring  8 trs\n",
      "25\n",
      "censoring  25 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "13\n",
      "censoring  13 trs\n",
      "41\n",
      "censoring  41 trs\n",
      "14\n",
      "censoring  14 trs\n",
      "19\n",
      "censoring  19 trs\n",
      "16\n",
      "censoring  16 trs\n",
      "27\n",
      "censoring  27 trs\n",
      "9\n",
      "censoring  9 trs\n",
      "10\n",
      "censoring  10 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "7\n",
      "censoring  7 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "15\n",
      "censoring  15 trs\n",
      "9\n",
      "censoring  9 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "17\n",
      "censoring  17 trs\n",
      "10\n",
      "censoring  10 trs\n",
      "7\n",
      "censoring  7 trs\n",
      "8\n",
      "censoring  8 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "0\n",
      "0\n",
      "21\n",
      "censoring  21 trs\n",
      "63\n",
      "censoring  63 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "33\n",
      "censoring  33 trs\n",
      "0\n",
      "10\n",
      "censoring  10 trs\n",
      "0\n",
      "4\n",
      "censoring  4 trs\n",
      "7\n",
      "censoring  7 trs\n",
      "14\n",
      "censoring  14 trs\n",
      "11\n",
      "censoring  11 trs\n",
      "5\n",
      "censoring  5 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "18\n",
      "censoring  18 trs\n",
      "14\n",
      "censoring  14 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "62\n",
      "censoring  62 trs\n",
      "14\n",
      "censoring  14 trs\n",
      "29\n",
      "censoring  29 trs\n",
      "39\n",
      "censoring  39 trs\n",
      "42\n",
      "censoring  42 trs\n",
      "43\n",
      "censoring  43 trs\n",
      "67\n",
      "censoring  67 trs\n",
      "86\n",
      "censoring  86 trs\n",
      "67\n",
      "censoring  67 trs\n",
      "65\n",
      "censoring  65 trs\n",
      "0\n",
      "0\n",
      "0\n",
      "32\n",
      "censoring  32 trs\n",
      "8\n",
      "censoring  8 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "11\n",
      "censoring  11 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "4\n",
      "censoring  4 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "0\n",
      "2\n",
      "censoring  2 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "27\n",
      "censoring  27 trs\n",
      "8\n",
      "censoring  8 trs\n",
      "5\n",
      "censoring  5 trs\n",
      "19\n",
      "censoring  19 trs\n",
      "16\n",
      "censoring  16 trs\n",
      "14\n",
      "censoring  14 trs\n",
      "4\n",
      "censoring  4 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "21\n",
      "censoring  21 trs\n",
      "67\n",
      "censoring  67 trs\n",
      "0\n",
      "0\n",
      "0\n",
      "26\n",
      "censoring  26 trs\n",
      "0\n",
      "38\n",
      "censoring  38 trs\n",
      "0\n",
      "0\n",
      "0\n",
      "9\n",
      "censoring  9 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "9\n",
      "censoring  9 trs\n",
      "4\n",
      "censoring  4 trs\n",
      "0\n",
      "33\n",
      "censoring  33 trs\n",
      "26\n",
      "censoring  26 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "0\n",
      "1\n",
      "censoring  1 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "censoring  1 trs\n",
      "0\n",
      "1\n",
      "censoring  1 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "4\n",
      "censoring  4 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "5\n",
      "censoring  5 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "19\n",
      "censoring  19 trs\n",
      "0\n",
      "34\n",
      "censoring  34 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "4\n",
      "censoring  4 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "censoring  1 trs\n",
      "0\n",
      "2\n",
      "censoring  2 trs\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "################## censor me timbers ##################\n",
    "\"\"\"\n",
    "fd_arr: array of ones and zeros, with ones being TRs above the threshold, length 209\n",
    "tot_cens: number of TRs censored per that run\n",
    "t_dic = fd_arr sorted by condition in a dictionary \n",
    "all_subs = dictionary of sorted sub censored TR data\n",
    "\"\"\"\n",
    "head_mot_dic = {}\n",
    "fd_arr = []\n",
    "cond_list = ['SM', \"SC\", \"RE\", 'OM', 'OC']\n",
    "all_subs = {}\n",
    "run_cens_list = []\n",
    "\n",
    "for sub in sub_list:\n",
    "    temp_dic = {}\n",
    "    t_dic = {}\n",
    "    lab_indic = label_indices(behav_p, sub)\n",
    "    for run in range(1,num_runs+1):\n",
    "        if sub == \"sub-002\":\n",
    "            if run >=7:\n",
    "                run = run+1\n",
    "        confounds_fn = pd.read_csv(os.path.join(data_dir, sub + \"/ses-01/func\",\"%s_ses-01_task-Attn_run-%s_desc-confounds_timeseries.tsv\" % (sub, run)),sep='\\t')\n",
    "        print(len(confounds_fn[confounds_fn['framewise_displacement'] > fd]['framewise_displacement']))\n",
    "        # Find percentage that were censored for each run at the current threshold\n",
    "        #tot_cens = len(confounds_fn[confounds_fn['framewise_displacement'] > fd]['framewise_displacement']) /209\n",
    "        #Find average framewsie displacement for the current run\n",
    "        tot_cens = np.nanmean(np.asarray(confounds_fn['framewise_displacement']))\n",
    "        # creat binary array\n",
    "        bin_arr = np.asarray(confounds_fn['framewise_displacement'] > fd).astype(int)\n",
    "        # find indices of censored TRs\n",
    "        cens_tr = np.where(bin_arr ==1)[0]\n",
    "        cens_mat = find_censTR_mat(cens_tr, tr_len=209)\n",
    "        run_cens_list.append(cens_mat) \n",
    "    # Above we aggregate the num censored into a list of 10. BELOW we SORT!\n",
    "    for cond in cond_list:\n",
    "        #print(cond)\n",
    "        run1 = run_cens_list[lab_indic[cond][0]]\n",
    "        run2 = run_cens_list[lab_indic[cond][1]]\n",
    "        t_dic[cond+\"-1\"] = run1\n",
    "        t_dic[cond+\"-2\"] = run2\n",
    "    all_subs[sub] = t_dic\n",
    "## only need to save once. all_subs will be used below\n",
    "#np.save(os.path.join(censor_dir, 'n28_numtrs_censored_at_threshp3.npy'),all_subs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-andorra",
   "metadata": {},
   "source": [
    "# Merge Sams HM matrix with censored @ .3 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "vertical-sandwich",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nall_subs: censored matrix of ones and zeroes, length 205\\nog_conf: sam's HM matrix (all confounds that we would normally insert)\\noutput, sub_dic is a full HM merged list of sams confounds + TRs to enclude\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now create a merged file for loading into the glm using data from above \n",
    "\"\"\"\n",
    "all_subs: censored matrix of ones and zeroes, length 205\n",
    "og_conf: sam's HM matrix (all confounds that we would normally insert)\n",
    "output, sub_dic is a full HM merged list of sams confounds + TRs to enclude\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "together-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub_list = ['sub-000']\n",
    "og_conf = load_confounds(cond_list, sub_list,behav_p, confounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "anticipated-emperor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-000\n",
      "sub-001\n",
      "sub-002\n",
      "sub-003\n",
      "sub-004\n",
      "sub-005\n",
      "sub-006\n",
      "sub-007\n",
      "sub-008\n",
      "sub-009\n",
      "sub-010\n",
      "sub-011\n",
      "sub-012\n",
      "sub-013\n",
      "sub-014\n",
      "sub-015\n",
      "sub-016\n",
      "sub-017\n",
      "sub-018\n",
      "sub-019\n",
      "sub-020\n",
      "sub-021\n",
      "sub-022\n",
      "sub-023\n",
      "sub-024\n",
      "sub-025\n",
      "sub-026\n",
      "sub-027\n"
     ]
    }
   ],
   "source": [
    "full_conf_sub = {}\n",
    "for sub in sub_list:\n",
    "    cond_dic = {}\n",
    "    for cond in cond_list:\n",
    "        run_arr = []\n",
    "        for run in np.arange(2):\n",
    "            # if no censoring, return old matrix\n",
    "            if all_subs[sub][cond+'-'+str(run+1)].shape[0] != 0:\n",
    "                og_conf_len = og_conf[sub][cond][run].shape[1]\n",
    "                cens_conf_len = all_subs[sub][cond+'-'+str(run+1)][4:,:].shape[1]\n",
    "                col_len =  og_conf_len + cens_conf_len\n",
    "                new_dat = np.zeros((205,col_len))\n",
    "                new_dat[:,:og_conf_len] = og_conf[sub][cond][run]\n",
    "                new_dat[:,og_conf_len:] = all_subs[sub][cond+'-'+str(run+1)][4:,:]\n",
    "                run_arr.append(new_dat)\n",
    "            else:\n",
    "                run_arr.append(og_conf[sub][cond][run])\n",
    "        cond_dic[cond] = run_arr\n",
    "    full_conf_sub[sub] = cond_dic \n",
    "    print(sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-albuquerque",
   "metadata": {},
   "source": [
    "# Exclude runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "responsible-paragraph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHow to use this part:\\n- not quite sure how the hell i created run_dic, but consists of all the runs to be included \\n    based on which subjects had button presses: np.array([0,1]) if both runs to be included \\n- full_conf_sub = full confound list to be condensed based on run exclusion\\n- tot_cens: n28_numtrs_censored_at_threshp3.npy: created using excel part below which sums TRs with more than .x head motion\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "How to use this part:\n",
    "- not quite sure how the hell i created run_dic, but consists of all the runs to be included \n",
    "    based on which subjects had button presses: np.array([0,1]) if both runs to be included \n",
    "- full_conf_sub = full confound list to be condensed based on run exclusion\n",
    "- tot_cens: n28_numtrs_censored_at_threshp3.npy: created using excel part below which sums TRs with more than .x head motion\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "internal-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change me ! and the files to load in below## \n",
    "## this is the confounds matrix ##\n",
    "out_name = 'n28_conf+cens_MERGE_removNoBpress_delHMruns_threshp'+thresh+'_glm.npy'\n",
    "## this is the runs to be included\n",
    "run_dic_name = 'n28_runs_2_include_removNoBpress_delHMruns_threshp'+thresh+'.npy'\n",
    "# out_name = 'test.npy'\n",
    "# ** may have to change this! could be 205 if some trs are on the borderline\n",
    "\n",
    "# what percentage trs can be excluded per run?\n",
    "excl = .3\n",
    "abov_thresh = int(209*excl)\n",
    "cond_list = ['SM', \"SC\", \"RE\", 'OM', 'OC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "residential-month",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/censor_hm'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "censor_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "unlike-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Confounds #\n",
    "#conf_sub = dict(enumerate(np.load(os.path.join(confounds_dir, 'n28_conf+cens_MERGE_10r_threshp%s_glm.npy') %(thresh), \n",
    "    #                                      allow_pickle = True).flatten(),1))[1]\n",
    "## full confound list is calculated above\n",
    "\n",
    "# full_conf_sub # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aerial-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load total TRs censored #\n",
    "tot_cen = dict(enumerate(np.load(os.path.join(confounds_dir, 'n28_numtrs_censored_at_threshp%s.npy')%(thresh), \n",
    "                                          allow_pickle = True).flatten(),1))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "hawaiian-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "## runs to include -- this was manually created to delete subs without button presses ## \n",
    "## we will merge this with other exclusion criteria\n",
    "run_dic = dict(enumerate(np.load(os.path.join(load_bpress, \"n28_runs_2_include.npy\"), \n",
    "                                allow_pickle=True).flatten(),1))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "virtual-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list = ['sub-019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "brazilian-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 1 ## \n",
    "# This loop will iterate through tot cens, which is a dictionary defining how many TRs are above a thresh\n",
    "# produces a cens_dat matrix consiting of which runs to include\n",
    "cens_dat = {}\n",
    "for sub in sub_list:\n",
    "    temp = {}\n",
    "    for cond in cond_list:\n",
    "        temp2= []\n",
    "        for run_num in range(2):\n",
    "            if tot_cen[sub][cond+\"-\"+str(run_num+1)] < abov_thresh:\n",
    "                temp2.append(run_num)\n",
    "        # save runs where the indices should be included according to run_dic\n",
    "        temp[cond] = np.asarray(temp2)\n",
    "    cens_dat[sub] = temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dimensional-foster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-019\n"
     ]
    }
   ],
   "source": [
    "## Part 2 ## \n",
    "# Remove runs without a button press AND runs above the censor threshold#\n",
    "# produces confound matrix with no bpress runs removed and censor threshold removed\n",
    "new_run_dic = {}\n",
    "conf_sub2 = {}\n",
    "for sub in sub_list:\n",
    "    print(sub)\n",
    "    temp = {}\n",
    "    temp_run_dic = {}\n",
    "    for cond in cond_list:\n",
    "        # select only the runs where there was a button press and above the mot thresh\n",
    "        intersect_arr = np.intersect1d(run_dic[sub][cond], cens_dat[sub][cond])\n",
    "        temp_run_dic[cond] = intersect_arr\n",
    "        # save runs where the indices should be included according to run_dic\n",
    "        temp[cond] = [full_conf_sub[sub][cond][i] for i in intersect_arr]\n",
    "        #print(conf_sub[sub][cond])\n",
    "    conf_sub2[sub] = temp\n",
    "    new_run_dic[sub] = temp_run_dic\n",
    "\n",
    "# save confounds\n",
    "#np.save(os.path.join(confounds_dir, out_name),conf_sub2)\n",
    "# Save new run dic that exludes runs with 30% of data gone\n",
    "#np.save(os.path.join(confounds_dir, run_dic_name),new_run_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "geographic-culture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sub-019': {'SM': array([0]),\n",
       "  'SC': array([0, 1]),\n",
       "  'RE': array([], dtype=float64),\n",
       "  'OM': array([], dtype=int64),\n",
       "  'OC': array([0])}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_run_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "challenging-shelf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SM': array([0, 1]),\n",
       " 'SC': array([0, 1]),\n",
       " 'OM': array([1]),\n",
       " 'OC': array([0, 1]),\n",
       " 'RE': array([0, 1])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_dic['sub-019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "adaptive-chrome",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-72e42a2186db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconf_sub2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sub-005'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "conf_sub2['sub-005']['SM'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-council",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-costs",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-breath",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-mercy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-harassment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is what i used for the excel doc, not for head motion censoring for mvpa, which is above and unfinished"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

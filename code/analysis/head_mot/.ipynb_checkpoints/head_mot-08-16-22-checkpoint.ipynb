{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "toxic-bottom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n8-3: implemented exclusion criteria based on percent TRs and whether or not there was no bpress\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#python resample.py $IMAGE_TO_RESAMPLE $REFERENCE_IMAGE\n",
    "\"\"\"\n",
    "8-3: implemented exclusion criteria based on percent TRs and whether or not there was no bpress\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "plastic-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter nbconvert --to python slurm_create-data_preproc.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brief-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "from nilearn.input_data import NiftiMasker , MultiNiftiMasker\n",
    "import nilearn as nil\n",
    "import numpy as np \n",
    "import os\n",
    "import os.path\n",
    "import scipy.io\n",
    "import nibabel as nib\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.masking import compute_epi_mask\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "import sys  \n",
    "import random\n",
    "# import logging\n",
    "\n",
    "import deepdish as dd\n",
    "import numpy as np\n",
    "\n",
    "import brainiak.eventseg.event\n",
    "import nibabel as nib\n",
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, zscore, pearsonr\n",
    "from scipy.signal import gaussian, convolve\n",
    "from sklearn import decomposition\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns \n",
    "\n",
    "#%autosave 5\n",
    "#%matplotlib inline\n",
    "sns.set(style = 'white', context='talk', font_scale=1, rc={\"lines.linewidth\": 2})\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "random.seed(10)\n",
    "#%matplotlib inline\n",
    "from brainiak import image, io\n",
    "from scipy.stats import stats\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from brainiak import image, io\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "import pandas as pd\n",
    "# Import machine learning libraries\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold, f_classif, SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import sem\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import statistics\n",
    "# Visualize it as an ROI\n",
    "from nilearn.plotting import plot_roi\n",
    "#plot_roi(x)\n",
    "from nilearn.image import concat_imgs, resample_img, mean_img\n",
    "from nilearn.plotting import view_img\n",
    "from nilearn import datasets, plotting\n",
    "from nilearn.input_data import NiftiSpheresMasker\n",
    "\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "from nilearn.image import concat_imgs, resample_img, mean_img,index_img\n",
    "from nilearn import image\n",
    "from nilearn import masking\n",
    "from nilearn.plotting import view_img\n",
    "from nilearn.image import resample_to_img\n",
    "import math\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-memory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sublime-kazakhstan",
   "metadata": {},
   "source": [
    "# Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-finish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "coordinate-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epi_data(sub, ses, task,run, space):\n",
    "  # Load MRI file\n",
    "    if space == \"MNI\":\n",
    "        epi_in = os.path.join(data_dir, sub, ses, 'func', \"%s_%s_task-%s_run-%s_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\" % (sub, ses, task,run))\n",
    "    elif space == \"T1\":\n",
    "        epi_in = os.path.join(data_dir, sub, ses, 'func', \"%s_%s_task-%s_run-%s_space-T1w_desc-preproc_bold.nii.gz\" % (sub, ses, task,run))\n",
    "    else:\n",
    "        print(\"wrong load epi input. check this function\")\n",
    "    epi_data = nib.load(epi_in)\n",
    "    print(epi_data.shape)\n",
    "    print(\"Loading data from %s\" % (epi_in))\n",
    "    return epi_data\n",
    "\n",
    "def load_roi_mask(ROI_name, space):\n",
    "    if space == \"MNI\":\n",
    "        maskdir = os.path.join(rois_dir)    \n",
    "        print(\"expected shape: 78, 93,65\")\n",
    "    elif space == \"T1\":\n",
    "        maskdir = os.path.join(rois_dir+ \"/T1\")\n",
    "        print(\"expected shape: 56, 72,53\")\n",
    "    else:\n",
    "        print(\"wrong mask input. check this function\")\n",
    "    # load the mask\n",
    "    maskfile = os.path.join(maskdir, \"%s.nii\" % (ROI_name))\n",
    "    mask = nib.load(maskfile)\n",
    "    print(\"mask shape: \", mask.shape)\n",
    "    print(\"Loaded %s mask\" % (ROI_name))\n",
    "    return mask\n",
    "def intersect_mask(sub, num_runs,reg, ses=\"ses-01\",task=\"Attn\"):\n",
    "    # This is based off of 'load_data' function in template\n",
    "    # Loads all fMRI runs into a matrix #\n",
    "    \"\"\"\n",
    "    reg = T1 or MNI registration?\n",
    "    norm_type = by Space or by Time? \n",
    "    \"\"\"\n",
    "    yoz = []\n",
    "    print(\"Begin intersecting, you sexy beast\")\n",
    "    for run in range(1, num_runs + 1):\n",
    "        if sub == \"sub-002\":\n",
    "            if run >=7:\n",
    "                run = run+1\n",
    "        # Load epi data \n",
    "        epi = load_epi_data(sub,ses,task,run,reg)\n",
    "        # Mask data\n",
    "        roi_samp = compute_epi_mask(epi) # -- whole brain\n",
    "        #roi_samp load_roi_mask(ROI_name,reg) # -- mask\n",
    "\n",
    "        nifti_masker = NiftiMasker(mask_img=roi_samp)\n",
    "        maskedData = nifti_masker.fit_transform(epi)\n",
    "        yoz.append(roi_samp)\n",
    "    #print(concatenated_data)\n",
    "    epi_data = nil.masking.intersect_masks(yoz)\n",
    "    print(\"all done wit da intersextion (lol)\")\n",
    "\n",
    "    return epi_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def label_lists(label, num_tr):\n",
    "    b = [[]]\n",
    "    a = []\n",
    "    for i in label:\n",
    "        # substring label in psychopy output\n",
    "        # if the first three characters == M_s, etc, then add correct indext to string\n",
    "        if i[1:4] == \"M_s\":\n",
    "            a.append(\"SM\")\n",
    "            b.append([0]*num_tr)\n",
    "        elif i[1:4] == \"C_s\":\n",
    "            a.append(\"SC\")\n",
    "            b.append([1]*num_tr)        \n",
    "        elif i[1:4] == \"M_o\":\n",
    "            a.append(\"OM\")\n",
    "            b.append([2]*num_tr)\n",
    "        elif i[1:4] == \"C_o\":\n",
    "            a.append(\"OC\")\n",
    "            b.append([3]*num_tr)     \n",
    "        else:\n",
    "            a.append(\"Re\")\n",
    "            b.append([4]*num_tr)     \n",
    "    return a, b[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-austin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "welcome-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fMRI3d(sub, num_runs,reg, norm_type, ses=\"ses-01\",task=\"Attn\"):\n",
    "    # This is based off of 'load_data' function in template\n",
    "    # Loads all fMRI runs into a matrix #\n",
    "    \"\"\"\n",
    "    reg = T1 or MNI registration?\n",
    "    norm_type = by Space or by Time? - default is by space (rows)\n",
    "    \"\"\"\n",
    "    concatenated_data = []\n",
    "    int_mask = intersect_mask(sub, 1,reg)\n",
    "    # \n",
    "    for run in range(1, num_runs + 1):\n",
    "        if sub == \"sub-002\":\n",
    "            if run >=7:\n",
    "                run = run+1\n",
    "        if sub != \"sub-010\":\n",
    "            # Load epi data \n",
    "            epi = load_epi_data(sub,ses,task,run,reg)\n",
    "        else:\n",
    "            # Load epi data \n",
    "            print(\"sub-10, watch out\")\n",
    "            bad_epi = load_epi_data(sub,ses,task,run,reg)\n",
    "            good_epi = load_epi_data(\"sub-001\",ses,task,run,reg)\n",
    "            epi = resample_to_img(bad_epi , good_epi, interpolation='nearest')\n",
    "            int_mask = resample_to_img(int_mask , good_epi, interpolation='nearest')\n",
    "        # delete first 9 TRs\n",
    "        epi = index_img(epi,slice(9,209))\n",
    "        \n",
    "        # load confounds\n",
    "        run_conf = np.asarray(pd.read_csv(os.path.join(confounds + sub + \"/func/\", \n",
    "                                                           '%s_ses-01_task-Attn_run-%s_desc-model_timeseries.csv') % (sub, run)))\n",
    "        print(run_conf.shape)\n",
    "        # clean image\n",
    "        # low_pass= .1,\n",
    "        clean_bold = image.clean_img(epi, standardize = False, confounds = run_conf[9:], high_pass=1/128, \n",
    "                                   t_r=1.5, mask_img = int_mask)\n",
    "        \n",
    "        \n",
    "        #Smooth\n",
    "        clean_bold = image.smooth_img(clean_bold, fwhm=5)\n",
    "        \n",
    "        # F*k it mask off -- Load ROI data\n",
    "        #roi_samp =load_roi_mask(ROI_name,reg)\n",
    "        # Pull voxels from epi data # *** may need to change this \n",
    "        #nifti_masker = NiftiMasker(mask_img=roi_samp)\n",
    "        #masked_data = nifti_masker.fit_transform(clean_bold)\n",
    "        \n",
    "        #append to cat_dat\n",
    "        concatenated_data.append(clean_bold)\n",
    "    \"FINISHED YAY BEAST\"\n",
    "    return concatenated_data, epi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "better-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fMRI3d(sub, num_runs,reg, norm_type, mask, ses=\"ses-01\",task=\"Attn\"):\n",
    "    # This is based off of 'load_data' function in template\n",
    "    # Loads all fMRI runs into a matrix #\n",
    "    \"\"\"\n",
    "    reg = T1 or MNI registration?\n",
    "    norm_type = by Space or by Time? - default is by space (rows)\n",
    "    \"\"\"\n",
    "    concatenated_data = []\n",
    "    \n",
    "    for run in range(1, num_runs + 1):\n",
    "        if sub == \"sub-002\":\n",
    "            if run >=7:\n",
    "                run = run+1\n",
    "        if sub != \"sub-010\":\n",
    "            # Load epi data \n",
    "            epi = load_epi_data(sub,ses,task,run,reg)\n",
    "        else:\n",
    "            # Load epi data \n",
    "            print(\"sub-10, watch out\")\n",
    "            bad_epi = load_epi_data(sub,ses,task,run,reg)\n",
    "            good_epi = load_epi_data(\"sub-001\",ses,task,run,reg)\n",
    "            epi = resample_to_img(bad_epi , good_epi, interpolation='nearest')\n",
    "        # delete first 9 TRs\n",
    "        epi = index_img(epi,slice(9,209))\n",
    "        \n",
    "        # load confounds\n",
    "        run_conf = np.asarray(pd.read_csv(os.path.join(confounds + sub + \"/func/\", \n",
    "                                                           '%s_ses-01_task-Attn_run-%s_desc-model_timeseries.csv') % (sub, run)))\n",
    "        print(run_conf.shape)\n",
    "        # clean image\n",
    "        # low_pass= .1,\n",
    "        clean_bold = image.clean_img(epi, standardize = False, confounds = run_conf[9:], high_pass=1/128, \n",
    "                                   t_r=1.5, mask_img = mask)\n",
    "        \n",
    "        \n",
    "        #Smooth\n",
    "        clean_bold = image.smooth_img(clean_bold, fwhm=5)\n",
    "        \n",
    "        # F*k it mask off -- Load ROI data\n",
    "        #roi_samp =load_roi_mask(ROI_name,reg)\n",
    "        # Pull voxels from epi data # *** may need to change this \n",
    "        #nifti_masker = NiftiMasker(mask_img=roi_samp)\n",
    "        #masked_data = nifti_masker.fit_transform(clean_bold)\n",
    "        \n",
    "        #append to cat_dat\n",
    "        concatenated_data.append(clean_bold)\n",
    "    \"FINISHED YAY BEAST\"\n",
    "    return concatenated_data, epi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "brilliant-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_indices(behav_p, sub):\n",
    "    behav = pd.read_csv(os.path.join(behav_p, '%s_behav_cleaned.csv') % (sub))\n",
    "    # Define the column in behav to be used for creating labels # \n",
    "    label = behav.iloc[:,1]\n",
    "    # Create an array of labels [1] AND the order in which runs occured [0]#\n",
    "    sub_ses_labels = label_lists(label, 200)\n",
    "    ## Find run sequence, extraction condition indexes from behav data ## \n",
    "    return find_cond_index(sub_ses_labels[0])\n",
    "\n",
    "def find_cond_index(sub_ses_labels):\n",
    "    \"\"\"\n",
    "    For the array of ordered run names (i.e.'Re', 'SM',) find the two indexes per condition\n",
    "    \"\"\" \n",
    "    lab_inx = []\n",
    "\n",
    "    a = []\n",
    "    b = []\n",
    "    c = []\n",
    "    d = []\n",
    "\n",
    "    for i in enumerate(sub_ses_labels):\n",
    "        if i[1] == \"SM\":\n",
    "            # append the index according to where it appeared in the array\n",
    "            a.append(i[0])\n",
    "        if i[1] == \"SC\":\n",
    "            b.append(i[0])\n",
    "        if i[1] == \"OM\":\n",
    "            c.append(i[0])\n",
    "        if i[1] == \"OC\":\n",
    "            d.append(i[0])\n",
    "\n",
    "    # Create a dictionary where each key contains the appropriate indexes\n",
    "    lab_indic = {\n",
    "        'SM' : a,\n",
    "        'SC' : b,\n",
    "        'OM' : c,\n",
    "        'OC' : d,\n",
    "        'RE' : [0,9]\n",
    "    }\n",
    "    return lab_indic \n",
    "    #np.vstack(lab_inx, [\"SM\", \"SC\", \"OM\", \"OC\"])\n",
    "    \n",
    "\n",
    "def load_confounds(cond_list, sub_list,behav_p,confounds):\n",
    "    \"\"\"\n",
    "    args: \n",
    "        cond_list: list of conditions (cond_list=np.array(['SM','SC']))\n",
    "        sub_list: subjects to extract confounds for\n",
    "        behav_p: path to the behavioral data\n",
    "        confounds: path to the confound data\n",
    "    returns:\n",
    "        nested dictionary in the form of: conf_sub[sub][cond][img_ind]\n",
    "        where img_index is the first or second run\n",
    "    \"\"\"\n",
    "    # Confound files\n",
    "\n",
    "    conf_sub = {}\n",
    "    for sub in sub_list:\n",
    "        conf_cond = {}\n",
    "        for cond in cond_list:\n",
    "            confs = []\n",
    "            lab_indic = fnd_indices(sub, behav_p)\n",
    "            ## plus one because indices are 0 thru 9 ## \n",
    "            run_1_ind = lab_indic[cond][0] +1\n",
    "            run_2_ind = lab_indic[cond][1] +1\n",
    "            if sub == 'sub-002':\n",
    "                if run_1_ind >=7 : run_1_ind +=1\n",
    "                if run_2_ind >=7 : run_2_ind +=1\n",
    "            \n",
    "            # Confound load #    \n",
    "            confs.append(np.asarray(pd.read_csv(os.path.join(confounds + sub + \"/func/\", \n",
    "                                                             '%s_ses-01_task-Attn_run-%s_desc-model_timeseries.csv') % (sub, run_1_ind)))[4:,:])\n",
    "            confs.append(np.asarray(pd.read_csv(os.path.join(confounds + sub + \"/func/\",\n",
    "                                                             '%s_ses-01_task-Attn_run-%s_desc-model_timeseries.csv') % (sub, run_2_ind)))[4:,:])\n",
    "            conf_cond[cond] = confs\n",
    "        conf_sub[sub] = conf_cond\n",
    "    return conf_sub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "crucial-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnd_indices(sub,behav_p):\n",
    "    behav = pd.read_csv(os.path.join(behav_p, '%s_behav_cleaned.csv') % (sub))\n",
    "    # Define the column in behav to be used for creating labels # \n",
    "    label = behav.iloc[:,1]\n",
    "    # Create an array of labels [1] AND the order in which runs occured [0]#\n",
    "    sub_ses_labels = label_lists(label, 200)\n",
    "    ## Find run sequence, extraction condition indexes from behav data ## \n",
    "    return find_cond_index(sub_ses_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-network",
   "metadata": {},
   "source": [
    "# Define Static VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "improving-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "data_dir = \"/jukebox/graziano/coolCatIsaac/ATM/data/bids/derivatives/fmriprep/\"\n",
    "rois_dir = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/rois/\"\n",
    "behav_p = '/jukebox/graziano/coolCatIsaac/ATM/data/behavioral'\n",
    "sav_work = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/results/corr_data/\"\n",
    "load_work = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/results/corr_data/\"\n",
    "confounds = '/jukebox/graziano/coolCatIsaac/ATM/data/bids/derivatives/fmriprep/afni-head_mot/'\n",
    "workspace = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/\"\n",
    "parc_dir = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/rois/schaef_par/MNI/\"\n",
    "sav_fcma = '/jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/load_fcma'\n",
    "censor_dir = '/jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/censor_hm'\n",
    "utils = '/jukebox/graziano/coolCatIsaac/ATM/code/analysis/bpress/utils.py'\n",
    "load_bpress = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/results/bpress_GLM/behav\"\n",
    "confounds_dir = '/jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/censor_hm/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "treated-discharge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/censor_hm'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "censor_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "flexible-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import fnd_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-exception",
   "metadata": {},
   "source": [
    "# lesss do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "convenient-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GENERAL ####\n",
    "sub_list = [\"sub-000\",\"sub-001\",\"sub-003\",\"sub-004\",\"sub-005\",\"sub-006\",\"sub-007\",\"sub-008\",\"sub-009\",\n",
    "            \"sub-011\",\"sub-012\",\"sub-013\",\"sub-014\"]\n",
    "sub_list = [\"sub-000\",\"sub-001\",\"sub-005\",\"sub-006\",\"sub-007\",\"sub-008\",\"sub-009\",\n",
    "            \"sub-011\",\"sub-012\",\"sub-013\",\"sub-014\"]\n",
    "sub_list = [\"sub-015\", \"sub-016\",\"sub-017\", \"sub-018\", \"sub-019\", \"sub-020\",\"sub-021\", \"sub-021\"]\n",
    "sub_list = [\"sub-000\",\"sub-001\",\"sub-002\",\"sub-003\",\"sub-004\",\"sub-005\",\"sub-006\",\"sub-007\",\"sub-008\",\"sub-009\",\n",
    "            \"sub-010\",\"sub-011\",\"sub-012\",\"sub-013\",\"sub-014\",\"sub-015\", \"sub-016\",\"sub-017\", \n",
    "            \"sub-018\", \"sub-019\", \"sub-020\",\"sub-021\"]\n",
    "\"\"\"\n",
    "sub_list = [\"sub-011\",\"sub-012\",\"sub-013\",\"sub-014\",\"sub-015\", \"sub-016\",\"sub-017\", \n",
    "            \"sub-018\", \"sub-019\", \"sub-020\",\"sub-021\"]\n",
    "            \n",
    "\"\"\"\n",
    "sub_list = [\"sub-000\",\"sub-001\",\"sub-002\",\"sub-003\",\"sub-004\",\"sub-005\",\"sub-006\",\"sub-007\",\"sub-008\",\"sub-009\",\n",
    "            \"sub-010\",\"sub-011\",\"sub-012\",\"sub-013\",\"sub-014\",\"sub-015\", \"sub-016\",\"sub-017\", \n",
    "            \"sub-018\", \"sub-019\", \"sub-020\",\"sub-021\",'sub-022','sub-023','sub-024','sub-025','sub-026','sub-027']\n",
    "#sub_list = [\"sub-005\"]\n",
    "\n",
    "# censor threshold\n",
    "num_runs = 10\n",
    "fd = .3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-excess",
   "metadata": {},
   "source": [
    "# Head motion data Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "attempted-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = pd.read_csv('/jukebox/graziano/coolCatIsaac/ATM/data/bids/derivatives/fmriprep/sub-000/ses-01/func/sub-000_ses-01_task-Attn_run-1_desc-confounds_timeseries.tsv',sep='\\t')\n",
    "y = np.load(os.path.join(censor_dir, 'sub-000_censor_10r.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "muslim-knife",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.T[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-rescue",
   "metadata": {},
   "source": [
    "# One sub censor matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "changed-pathology",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "36\n",
      "45\n",
      "23\n",
      "5\n",
      "9\n",
      "34\n",
      "43\n",
      "16\n",
      "12\n",
      "3\n",
      "9\n",
      "10\n",
      "31\n",
      "23\n",
      "9\n",
      "34\n",
      "40\n",
      "29\n",
      "12\n",
      "9\n",
      "61\n",
      "32\n",
      "19\n",
      "7\n",
      "36\n",
      "12\n",
      "14\n",
      "82\n",
      "11\n",
      "8\n",
      "6\n",
      "32\n",
      "34\n",
      "13\n",
      "30\n",
      "31\n",
      "46\n",
      "33\n",
      "49\n",
      "5\n",
      "0\n",
      "10\n",
      "14\n",
      "19\n",
      "17\n",
      "23\n",
      "8\n",
      "18\n",
      "6\n",
      "0\n",
      "4\n",
      "80\n",
      "38\n",
      "92\n",
      "9\n",
      "56\n",
      "134\n",
      "116\n",
      "8\n",
      "2\n",
      "6\n",
      "10\n",
      "66\n",
      "56\n",
      "6\n",
      "51\n",
      "77\n",
      "18\n",
      "14\n",
      "0\n",
      "19\n",
      "17\n",
      "38\n",
      "20\n",
      "89\n",
      "85\n",
      "19\n",
      "86\n",
      "1\n",
      "0\n",
      "1\n",
      "47\n",
      "7\n",
      "91\n",
      "2\n",
      "68\n",
      "69\n",
      "15\n",
      "0\n",
      "2\n",
      "2\n",
      "6\n",
      "7\n",
      "6\n",
      "14\n",
      "5\n",
      "0\n",
      "2\n",
      "4\n",
      "12\n",
      "6\n",
      "56\n",
      "28\n",
      "8\n",
      "95\n",
      "41\n",
      "32\n",
      "63\n",
      "10\n",
      "7\n",
      "6\n",
      "56\n",
      "67\n",
      "28\n",
      "34\n",
      "33\n",
      "29\n",
      "1\n",
      "2\n",
      "60\n",
      "59\n",
      "57\n",
      "42\n",
      "69\n",
      "46\n",
      "77\n",
      "80\n",
      "73\n",
      "64\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "10\n",
      "4\n",
      "0\n",
      "2\n",
      "5\n",
      "8\n",
      "6\n",
      "42\n",
      "2\n",
      "2\n",
      "7\n",
      "13\n",
      "8\n",
      "25\n",
      "6\n",
      "13\n",
      "41\n",
      "14\n",
      "19\n",
      "16\n",
      "27\n",
      "9\n",
      "10\n",
      "2\n",
      "7\n",
      "2\n",
      "15\n",
      "9\n",
      "3\n",
      "6\n",
      "17\n",
      "10\n",
      "7\n",
      "8\n",
      "2\n",
      "0\n",
      "0\n",
      "21\n",
      "63\n",
      "3\n",
      "33\n",
      "0\n",
      "10\n",
      "0\n",
      "4\n",
      "7\n",
      "14\n",
      "11\n",
      "5\n",
      "1\n",
      "18\n",
      "14\n",
      "6\n",
      "1\n",
      "62\n",
      "14\n",
      "29\n",
      "39\n",
      "42\n",
      "43\n",
      "67\n",
      "86\n",
      "67\n",
      "65\n",
      "0\n",
      "0\n",
      "0\n",
      "32\n",
      "8\n",
      "3\n",
      "11\n",
      "3\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "27\n",
      "8\n",
      "5\n",
      "19\n",
      "16\n",
      "14\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "################## censor me timbers ##################\n",
    "\n",
    "head_mot_dic = {}\n",
    "fd_arr = []\n",
    "cond_list = ['SM', \"SC\", \"RE\", 'OM', 'OC']\n",
    "all_subs = {}\n",
    "\n",
    "for sub in sub_list:\n",
    "    temp_dic = {}\n",
    "    t_dic = {}\n",
    "    lab_indic = label_indices(behav_p, sub)\n",
    "    for run in range(1,num_runs+1):\n",
    "        if sub == \"sub-002\":\n",
    "            if run >=7:\n",
    "                run = run+1\n",
    "        confounds_fn = pd.read_csv(os.path.join(data_dir, sub + \"/ses-01/func\",\"%s_ses-01_task-Attn_run-%s_desc-confounds_timeseries.tsv\" % (sub, run)),sep='\\t')\n",
    "        print(len(confounds_fn[confounds_fn['framewise_displacement'] > fd]['framewise_displacement']))\n",
    "        tot_cens = len(confounds_fn[confounds_fn['framewise_displacement'] > fd]['framewise_displacement'])\n",
    "        # creat binary array\n",
    "        fd_arr.append(np.asarray(confounds_fn['framewise_displacement'] > fd).astype(int))\n",
    "        \n",
    "    #np.save(os.path.join(censor_dir, '%s_censor_10r_TEST.npy') % (sub),fd_arr)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-respect",
   "metadata": {},
   "source": [
    "# Find the censor matrix of ones for each censored TR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "answering-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_censTR_mat(cens_tr, tr_len=209):\n",
    "    \"\"\"\n",
    "    args: \n",
    "        cens_tr: indices of censored TRs\n",
    "    \"\"\"\n",
    "    if cens_tr.shape[0] == 0:\n",
    "        return cens_tr\n",
    "    else:\n",
    "        cens_mat = np.zeros((tr_len, int(len(cens_tr))))\n",
    "        for idx, tr in enumerate(cens_tr):\n",
    "            cens_col = np.zeros((tr_len))\n",
    "            cens_col[tr] = 1\n",
    "            cens_mat[:,idx] = cens_col\n",
    "        print(\"censoring \", cens_mat.shape[1], 'trs')\n",
    "    return cens_mat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "powered-bonus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "censoring  27 trs\n",
      "36\n",
      "censoring  36 trs\n",
      "45\n",
      "censoring  45 trs\n",
      "23\n",
      "censoring  23 trs\n",
      "5\n",
      "censoring  5 trs\n",
      "9\n",
      "censoring  9 trs\n",
      "34\n",
      "censoring  34 trs\n",
      "43\n",
      "censoring  43 trs\n",
      "16\n",
      "censoring  16 trs\n",
      "12\n",
      "censoring  12 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "9\n",
      "censoring  9 trs\n",
      "10\n",
      "censoring  10 trs\n",
      "31\n",
      "censoring  31 trs\n",
      "23\n",
      "censoring  23 trs\n",
      "9\n",
      "censoring  9 trs\n",
      "34\n",
      "censoring  34 trs\n",
      "40\n",
      "censoring  40 trs\n",
      "29\n",
      "censoring  29 trs\n",
      "12\n",
      "censoring  12 trs\n",
      "9\n",
      "censoring  9 trs\n",
      "61\n",
      "censoring  61 trs\n",
      "32\n",
      "censoring  32 trs\n",
      "19\n",
      "censoring  19 trs\n",
      "7\n",
      "censoring  7 trs\n",
      "36\n",
      "censoring  36 trs\n",
      "12\n",
      "censoring  12 trs\n",
      "14\n",
      "censoring  14 trs\n",
      "82\n",
      "censoring  82 trs\n",
      "11\n",
      "censoring  11 trs\n",
      "8\n",
      "censoring  8 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "32\n",
      "censoring  32 trs\n",
      "34\n",
      "censoring  34 trs\n",
      "13\n",
      "censoring  13 trs\n",
      "30\n",
      "censoring  30 trs\n",
      "31\n",
      "censoring  31 trs\n",
      "46\n",
      "censoring  46 trs\n",
      "33\n",
      "censoring  33 trs\n",
      "49\n",
      "censoring  49 trs\n",
      "5\n",
      "censoring  5 trs\n",
      "0\n",
      "10\n",
      "censoring  10 trs\n",
      "14\n",
      "censoring  14 trs\n",
      "19\n",
      "censoring  19 trs\n",
      "17\n",
      "censoring  17 trs\n",
      "23\n",
      "censoring  23 trs\n",
      "8\n",
      "censoring  8 trs\n",
      "18\n",
      "censoring  18 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "0\n",
      "4\n",
      "censoring  4 trs\n",
      "80\n",
      "censoring  80 trs\n",
      "38\n",
      "censoring  38 trs\n",
      "92\n",
      "censoring  92 trs\n",
      "9\n",
      "censoring  9 trs\n",
      "56\n",
      "censoring  56 trs\n",
      "134\n",
      "censoring  134 trs\n",
      "116\n",
      "censoring  116 trs\n",
      "8\n",
      "censoring  8 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "10\n",
      "censoring  10 trs\n",
      "66\n",
      "censoring  66 trs\n",
      "56\n",
      "censoring  56 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "51\n",
      "censoring  51 trs\n",
      "77\n",
      "censoring  77 trs\n",
      "18\n",
      "censoring  18 trs\n",
      "14\n",
      "censoring  14 trs\n",
      "0\n",
      "19\n",
      "censoring  19 trs\n",
      "17\n",
      "censoring  17 trs\n",
      "38\n",
      "censoring  38 trs\n",
      "20\n",
      "censoring  20 trs\n",
      "89\n",
      "censoring  89 trs\n",
      "85\n",
      "censoring  85 trs\n",
      "19\n",
      "censoring  19 trs\n",
      "86\n",
      "censoring  86 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "0\n",
      "1\n",
      "censoring  1 trs\n",
      "47\n",
      "censoring  47 trs\n",
      "7\n",
      "censoring  7 trs\n",
      "91\n",
      "censoring  91 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "68\n",
      "censoring  68 trs\n",
      "69\n",
      "censoring  69 trs\n",
      "15\n",
      "censoring  15 trs\n",
      "0\n",
      "2\n",
      "censoring  2 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "7\n",
      "censoring  7 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "14\n",
      "censoring  14 trs\n",
      "5\n",
      "censoring  5 trs\n",
      "0\n",
      "2\n",
      "censoring  2 trs\n",
      "4\n",
      "censoring  4 trs\n",
      "12\n",
      "censoring  12 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "56\n",
      "censoring  56 trs\n",
      "28\n",
      "censoring  28 trs\n",
      "8\n",
      "censoring  8 trs\n",
      "95\n",
      "censoring  95 trs\n",
      "41\n",
      "censoring  41 trs\n",
      "32\n",
      "censoring  32 trs\n",
      "63\n",
      "censoring  63 trs\n",
      "10\n",
      "censoring  10 trs\n",
      "7\n",
      "censoring  7 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "56\n",
      "censoring  56 trs\n",
      "67\n",
      "censoring  67 trs\n",
      "28\n",
      "censoring  28 trs\n",
      "34\n",
      "censoring  34 trs\n",
      "33\n",
      "censoring  33 trs\n",
      "29\n",
      "censoring  29 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "60\n",
      "censoring  60 trs\n",
      "59\n",
      "censoring  59 trs\n",
      "57\n",
      "censoring  57 trs\n",
      "42\n",
      "censoring  42 trs\n",
      "69\n",
      "censoring  69 trs\n",
      "46\n",
      "censoring  46 trs\n",
      "77\n",
      "censoring  77 trs\n",
      "80\n",
      "censoring  80 trs\n",
      "73\n",
      "censoring  73 trs\n",
      "64\n",
      "censoring  64 trs\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "censoring  3 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "10\n",
      "censoring  10 trs\n",
      "4\n",
      "censoring  4 trs\n",
      "0\n",
      "2\n",
      "censoring  2 trs\n",
      "5\n",
      "censoring  5 trs\n",
      "8\n",
      "censoring  8 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "42\n",
      "censoring  42 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "7\n",
      "censoring  7 trs\n",
      "13\n",
      "censoring  13 trs\n",
      "8\n",
      "censoring  8 trs\n",
      "25\n",
      "censoring  25 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "13\n",
      "censoring  13 trs\n",
      "41\n",
      "censoring  41 trs\n",
      "14\n",
      "censoring  14 trs\n",
      "19\n",
      "censoring  19 trs\n",
      "16\n",
      "censoring  16 trs\n",
      "27\n",
      "censoring  27 trs\n",
      "9\n",
      "censoring  9 trs\n",
      "10\n",
      "censoring  10 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "7\n",
      "censoring  7 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "15\n",
      "censoring  15 trs\n",
      "9\n",
      "censoring  9 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "17\n",
      "censoring  17 trs\n",
      "10\n",
      "censoring  10 trs\n",
      "7\n",
      "censoring  7 trs\n",
      "8\n",
      "censoring  8 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "0\n",
      "0\n",
      "21\n",
      "censoring  21 trs\n",
      "63\n",
      "censoring  63 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "33\n",
      "censoring  33 trs\n",
      "0\n",
      "10\n",
      "censoring  10 trs\n",
      "0\n",
      "4\n",
      "censoring  4 trs\n",
      "7\n",
      "censoring  7 trs\n",
      "14\n",
      "censoring  14 trs\n",
      "11\n",
      "censoring  11 trs\n",
      "5\n",
      "censoring  5 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "18\n",
      "censoring  18 trs\n",
      "14\n",
      "censoring  14 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "62\n",
      "censoring  62 trs\n",
      "14\n",
      "censoring  14 trs\n",
      "29\n",
      "censoring  29 trs\n",
      "39\n",
      "censoring  39 trs\n",
      "42\n",
      "censoring  42 trs\n",
      "43\n",
      "censoring  43 trs\n",
      "67\n",
      "censoring  67 trs\n",
      "86\n",
      "censoring  86 trs\n",
      "67\n",
      "censoring  67 trs\n",
      "65\n",
      "censoring  65 trs\n",
      "0\n",
      "0\n",
      "0\n",
      "32\n",
      "censoring  32 trs\n",
      "8\n",
      "censoring  8 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "11\n",
      "censoring  11 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "4\n",
      "censoring  4 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "0\n",
      "2\n",
      "censoring  2 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "27\n",
      "censoring  27 trs\n",
      "8\n",
      "censoring  8 trs\n",
      "5\n",
      "censoring  5 trs\n",
      "19\n",
      "censoring  19 trs\n",
      "16\n",
      "censoring  16 trs\n",
      "14\n",
      "censoring  14 trs\n",
      "4\n",
      "censoring  4 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "21\n",
      "censoring  21 trs\n",
      "67\n",
      "censoring  67 trs\n",
      "0\n",
      "0\n",
      "0\n",
      "26\n",
      "censoring  26 trs\n",
      "0\n",
      "38\n",
      "censoring  38 trs\n",
      "0\n",
      "0\n",
      "0\n",
      "9\n",
      "censoring  9 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "9\n",
      "censoring  9 trs\n",
      "4\n",
      "censoring  4 trs\n",
      "0\n",
      "33\n",
      "censoring  33 trs\n",
      "26\n",
      "censoring  26 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "0\n",
      "1\n",
      "censoring  1 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "censoring  1 trs\n",
      "0\n",
      "1\n",
      "censoring  1 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "4\n",
      "censoring  4 trs\n",
      "6\n",
      "censoring  6 trs\n",
      "5\n",
      "censoring  5 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "19\n",
      "censoring  19 trs\n",
      "0\n",
      "34\n",
      "censoring  34 trs\n",
      "1\n",
      "censoring  1 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "2\n",
      "censoring  2 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "4\n",
      "censoring  4 trs\n",
      "3\n",
      "censoring  3 trs\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "censoring  1 trs\n",
      "0\n",
      "2\n",
      "censoring  2 trs\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "################## censor me timbers ##################\n",
    "\"\"\"\n",
    "fd_arr: array of ones and zeros, with ones being TRs above the threshold, length 209\n",
    "tot_cens: number of TRs censored per that run\n",
    "t_dic = fd_arr sorted by condition in a dictionary \n",
    "all_subs = dictionary of sorted sub censored TR data\n",
    "\"\"\"\n",
    "head_mot_dic = {}\n",
    "fd_arr = []\n",
    "cond_list = ['SM', \"SC\", \"RE\", 'OM', 'OC']\n",
    "all_subs = {}\n",
    "run_cens_list = []\n",
    "\n",
    "for sub in sub_list:\n",
    "    temp_dic = {}\n",
    "    t_dic = {}\n",
    "    lab_indic = label_indices(behav_p, sub)\n",
    "    for run in range(1,num_runs+1):\n",
    "        if sub == \"sub-002\":\n",
    "            if run >=7:\n",
    "                run = run+1\n",
    "        confounds_fn = pd.read_csv(os.path.join(data_dir, sub + \"/ses-01/func\",\"%s_ses-01_task-Attn_run-%s_desc-confounds_timeseries.tsv\" % (sub, run)),sep='\\t')\n",
    "        print(len(confounds_fn[confounds_fn['framewise_displacement'] > fd]['framewise_displacement']))\n",
    "        # Find percentage that were censored for each run at the current threshold\n",
    "        #tot_cens = len(confounds_fn[confounds_fn['framewise_displacement'] > fd]['framewise_displacement']) /209\n",
    "        #Find average framewsie displacement for the current run\n",
    "        tot_cens = np.nanmean(np.asarray(confounds_fn['framewise_displacement']))\n",
    "        # creat binary array\n",
    "        bin_arr = np.asarray(confounds_fn['framewise_displacement'] > fd).astype(int)\n",
    "        # find indices of censored TRs\n",
    "        cens_tr = np.where(bin_arr ==1)[0]\n",
    "        cens_mat = find_censTR_mat(cens_tr, tr_len=209)\n",
    "        run_cens_list.append(cens_mat) \n",
    "    # Above we aggregate the num censored into a list of 10. BELOW we SORT!\n",
    "    for cond in cond_list:\n",
    "        #print(cond)\n",
    "        run1 = run_cens_list[lab_indic[cond][0]]\n",
    "        run2 = run_cens_list[lab_indic[cond][1]]\n",
    "        t_dic[cond+\"-1\"] = run1\n",
    "        t_dic[cond+\"-2\"] = run2\n",
    "    all_subs[sub] = t_dic\n",
    "#np.save(os.path.join(censor_dir, 'n28_censor_10r_tr_matrix_glm.npy'),all_subs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "multiple-assumption",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SM-1': array([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]]),\n",
       " 'SM-2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'SC-1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'SC-2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'RE-1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'RE-2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'OM-1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'OM-2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'OC-1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'OC-2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subs['sub-005']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-folks",
   "metadata": {},
   "source": [
    "# Merge Sams HM matrix with censored @ .3 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "opposite-greeting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nall_subs: @.3 censored matrix\\nog_conf: sam's HM matrix (all confounds that we would normally insert)\\n\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now create a merged file for loading into the glm using data from above \n",
    "\"\"\"\n",
    "all_subs: @.3 censored matrix\n",
    "og_conf: sam's HM matrix (all confounds that we would normally insert)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "tested-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub_list = ['sub-000']\n",
    "og_conf = load_confounds(cond_list, sub_list,behav_p, confounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "false-bundle",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-000\n",
      "sub-001\n",
      "sub-002\n",
      "sub-003\n",
      "sub-004\n",
      "sub-005\n",
      "sub-006\n",
      "sub-007\n",
      "sub-008\n",
      "sub-009\n",
      "sub-010\n",
      "sub-011\n",
      "sub-012\n",
      "sub-013\n",
      "sub-014\n",
      "sub-015\n",
      "sub-016\n",
      "sub-017\n",
      "sub-018\n",
      "sub-019\n",
      "sub-020\n",
      "sub-021\n",
      "sub-022\n",
      "sub-023\n",
      "sub-024\n",
      "sub-025\n",
      "sub-026\n",
      "sub-027\n"
     ]
    }
   ],
   "source": [
    "sub_dic = {}\n",
    "for sub in sub_list:\n",
    "    cond_dic = {}\n",
    "    for cond in cond_list:\n",
    "        run_arr = []\n",
    "        for run in np.arange(2):\n",
    "            # if no censoring, return old matrix\n",
    "            if all_subs[sub][cond+'-'+str(run+1)].shape[0] != 0:\n",
    "                og_conf_len = og_conf[sub][cond][run].shape[1]\n",
    "                cens_conf_len = all_subs[sub][cond+'-'+str(run+1)][4:,:].shape[1]\n",
    "                col_len =  og_conf_len + cens_conf_len\n",
    "                new_dat = np.zeros((205,col_len))\n",
    "                new_dat[:,:og_conf_len] = og_conf[sub][cond][run]\n",
    "                new_dat[:,og_conf_len:] = all_subs[sub][cond+'-'+str(run+1)][4:,:]\n",
    "                run_arr.append(new_dat)\n",
    "            else:\n",
    "                run_arr.append(og_conf[sub][cond][run])\n",
    "        cond_dic[cond] = run_arr\n",
    "    sub_dic[sub] = cond_dic \n",
    "    print(sub)\n",
    "#np.save(os.path.join(censor_dir, 'n28_conf+cens_MERGE_10r_threshp5_glm.npy'),sub_dic)\n",
    "# NOT THE ONE BELOW... I think i had this for previous analyses\n",
    "#np.save(os.path.join(censor_dir, 'n28_conf+cens_MERGE_10r_threshp3_glm.npy'),all_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "smaller-match",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08712294,  0.        ,  0.        ],\n",
       "       [-0.0620691 ,  0.        ,  0.        ],\n",
       "       [ 0.0827948 ,  0.        ,  0.        ],\n",
       "       [-0.01903703,  0.        ,  0.        ],\n",
       "       [ 0.01438497,  0.        ,  0.        ],\n",
       "       [-0.01585497,  0.        ,  0.        ],\n",
       "       [ 0.02122305,  0.        ,  0.        ],\n",
       "       [ 0.01951727,  0.        ,  0.        ],\n",
       "       [ 0.02671062,  0.        ,  0.        ],\n",
       "       [ 0.00670354,  1.        ,  0.        ],\n",
       "       [-0.00244046,  0.        ,  0.        ],\n",
       "       [ 0.03514011,  0.        ,  0.        ],\n",
       "       [-0.07836285,  0.        ,  0.        ],\n",
       "       [-0.0148198 ,  0.        ,  0.        ],\n",
       "       [-0.09075008,  0.        ,  0.        ],\n",
       "       [ 0.05261745,  0.        ,  0.        ],\n",
       "       [-0.09607622,  0.        ,  0.        ],\n",
       "       [ 0.04561767,  0.        ,  0.        ],\n",
       "       [-0.00875862,  0.        ,  0.        ],\n",
       "       [-0.04105505,  0.        ,  0.        ],\n",
       "       [ 0.08875284,  0.        ,  0.        ],\n",
       "       [-0.01422562,  0.        ,  1.        ],\n",
       "       [ 0.00839789,  0.        ,  0.        ],\n",
       "       [ 0.02302161,  0.        ,  0.        ],\n",
       "       [ 0.00210055,  0.        ,  0.        ],\n",
       "       [ 0.04500693,  0.        ,  0.        ],\n",
       "       [-0.08407831,  0.        ,  0.        ],\n",
       "       [-0.02193854,  0.        ,  0.        ],\n",
       "       [ 0.0311408 ,  0.        ,  0.        ],\n",
       "       [-0.03460528,  0.        ,  0.        ],\n",
       "       [ 0.04557852,  0.        ,  0.        ],\n",
       "       [ 0.05760188,  0.        ,  0.        ],\n",
       "       [-0.02059638,  0.        ,  0.        ],\n",
       "       [ 0.15699461,  0.        ,  0.        ],\n",
       "       [-0.03413868,  0.        ,  0.        ],\n",
       "       [ 0.02993927,  0.        ,  0.        ],\n",
       "       [ 0.05685894,  0.        ,  0.        ],\n",
       "       [-0.08479729,  0.        ,  0.        ],\n",
       "       [ 0.02794228,  0.        ,  0.        ],\n",
       "       [-0.03984296,  0.        ,  0.        ],\n",
       "       [-0.00311533,  0.        ,  0.        ],\n",
       "       [ 0.03305021,  0.        ,  0.        ],\n",
       "       [ 0.01841142,  0.        ,  0.        ],\n",
       "       [-0.09418677,  0.        ,  0.        ],\n",
       "       [ 0.08006928,  0.        ,  0.        ],\n",
       "       [-0.07400863,  0.        ,  0.        ],\n",
       "       [ 0.02912011,  0.        ,  0.        ],\n",
       "       [-0.05008767,  0.        ,  0.        ],\n",
       "       [-0.03803821,  0.        ,  0.        ],\n",
       "       [-0.00927169,  0.        ,  0.        ],\n",
       "       [-0.10749972,  0.        ,  0.        ],\n",
       "       [-0.06446467,  0.        ,  0.        ],\n",
       "       [ 0.02499746,  0.        ,  0.        ],\n",
       "       [-0.12113806,  0.        ,  0.        ],\n",
       "       [ 0.01379125,  0.        ,  0.        ],\n",
       "       [-0.02210678,  0.        ,  0.        ],\n",
       "       [-0.08515608,  0.        ,  0.        ],\n",
       "       [ 0.01982647,  0.        ,  0.        ],\n",
       "       [-0.02786463,  0.        ,  0.        ],\n",
       "       [-0.11023105,  0.        ,  0.        ],\n",
       "       [ 0.03271285,  0.        ,  0.        ],\n",
       "       [-0.0948949 ,  0.        ,  0.        ],\n",
       "       [-0.00787371,  0.        ,  0.        ],\n",
       "       [-0.00856884,  0.        ,  0.        ],\n",
       "       [-0.08787668,  0.        ,  0.        ],\n",
       "       [-0.01725702,  0.        ,  0.        ],\n",
       "       [-0.17510103,  0.        ,  0.        ],\n",
       "       [ 0.05672032,  0.        ,  0.        ],\n",
       "       [-0.11048898,  0.        ,  0.        ],\n",
       "       [ 0.04562291,  0.        ,  0.        ],\n",
       "       [-0.02594316,  0.        ,  0.        ],\n",
       "       [ 0.00246384,  0.        ,  0.        ],\n",
       "       [ 0.03261137,  0.        ,  0.        ],\n",
       "       [-0.03812639,  0.        ,  0.        ],\n",
       "       [ 0.05230587,  0.        ,  0.        ],\n",
       "       [ 0.02865216,  0.        ,  0.        ],\n",
       "       [-0.01609229,  0.        ,  0.        ],\n",
       "       [ 0.1397365 ,  0.        ,  0.        ],\n",
       "       [-0.00541931,  0.        ,  0.        ],\n",
       "       [-0.00963007,  0.        ,  0.        ],\n",
       "       [ 0.08393418,  0.        ,  0.        ],\n",
       "       [ 0.07825672,  0.        ,  0.        ],\n",
       "       [-0.07949113,  0.        ,  0.        ],\n",
       "       [ 0.04703418,  0.        ,  0.        ],\n",
       "       [ 0.17071906,  0.        ,  0.        ],\n",
       "       [-0.01376846,  0.        ,  0.        ],\n",
       "       [ 0.02725189,  0.        ,  0.        ],\n",
       "       [ 0.09176452,  0.        ,  0.        ],\n",
       "       [-0.03180868,  0.        ,  0.        ],\n",
       "       [-0.00715073,  0.        ,  0.        ],\n",
       "       [ 0.0412793 ,  0.        ,  0.        ],\n",
       "       [-0.02031074,  0.        ,  0.        ],\n",
       "       [-0.03064768,  0.        ,  0.        ],\n",
       "       [ 0.09196933,  0.        ,  0.        ],\n",
       "       [-0.06241154,  0.        ,  0.        ],\n",
       "       [ 0.08757224,  0.        ,  0.        ],\n",
       "       [ 0.09153964,  0.        ,  0.        ],\n",
       "       [ 0.00416509,  0.        ,  0.        ],\n",
       "       [-0.04892117,  0.        ,  0.        ],\n",
       "       [ 0.14625751,  0.        ,  0.        ],\n",
       "       [-0.06789473,  0.        ,  0.        ],\n",
       "       [ 0.1321692 ,  0.        ,  0.        ],\n",
       "       [ 0.08703835,  0.        ,  0.        ],\n",
       "       [ 0.12009733,  0.        ,  0.        ],\n",
       "       [ 0.02399721,  0.        ,  0.        ],\n",
       "       [ 0.16837674,  0.        ,  0.        ],\n",
       "       [-0.06746736,  0.        ,  0.        ],\n",
       "       [ 0.12504931,  0.        ,  0.        ],\n",
       "       [-0.13427441,  0.        ,  0.        ],\n",
       "       [ 0.04168817,  0.        ,  0.        ],\n",
       "       [-0.07452309,  0.        ,  0.        ],\n",
       "       [-0.07427815,  0.        ,  0.        ],\n",
       "       [ 0.05722689,  0.        ,  0.        ],\n",
       "       [-0.07951995,  0.        ,  0.        ],\n",
       "       [-0.02836347,  0.        ,  0.        ],\n",
       "       [ 0.00891236,  0.        ,  0.        ],\n",
       "       [-0.1075867 ,  0.        ,  0.        ],\n",
       "       [ 0.01049453,  0.        ,  0.        ],\n",
       "       [ 0.0289985 ,  0.        ,  0.        ],\n",
       "       [-0.10419093,  0.        ,  0.        ],\n",
       "       [ 0.04033453,  0.        ,  0.        ],\n",
       "       [-0.01715812,  0.        ,  0.        ],\n",
       "       [-0.14269843,  0.        ,  0.        ],\n",
       "       [ 0.0398355 ,  0.        ,  0.        ],\n",
       "       [-0.08618011,  0.        ,  0.        ],\n",
       "       [-0.0954686 ,  0.        ,  0.        ],\n",
       "       [-0.01919257,  0.        ,  0.        ],\n",
       "       [ 0.04636983,  0.        ,  0.        ],\n",
       "       [-0.09020025,  0.        ,  0.        ],\n",
       "       [-0.02333507,  0.        ,  0.        ],\n",
       "       [ 0.01547655,  0.        ,  0.        ],\n",
       "       [ 0.01739157,  0.        ,  0.        ],\n",
       "       [-0.06309567,  0.        ,  0.        ],\n",
       "       [ 0.08228681,  0.        ,  0.        ],\n",
       "       [-0.03222516,  0.        ,  0.        ],\n",
       "       [-0.05235562,  0.        ,  0.        ],\n",
       "       [ 0.08983614,  0.        ,  0.        ],\n",
       "       [-0.00776385,  0.        ,  0.        ],\n",
       "       [-0.10368359,  0.        ,  0.        ],\n",
       "       [ 0.09331323,  0.        ,  0.        ],\n",
       "       [ 0.04122228,  0.        ,  0.        ],\n",
       "       [ 0.04950005,  0.        ,  0.        ],\n",
       "       [ 0.07429478,  0.        ,  0.        ],\n",
       "       [ 0.05684254,  0.        ,  0.        ],\n",
       "       [-0.09356901,  0.        ,  0.        ],\n",
       "       [ 0.12542224,  0.        ,  0.        ],\n",
       "       [-0.1074895 ,  0.        ,  0.        ],\n",
       "       [ 0.04875285,  0.        ,  0.        ],\n",
       "       [-0.01516775,  0.        ,  0.        ],\n",
       "       [-0.05596804,  0.        ,  0.        ],\n",
       "       [-0.00257217,  0.        ,  0.        ],\n",
       "       [ 0.0167494 ,  0.        ,  0.        ],\n",
       "       [-0.11638512,  0.        ,  0.        ],\n",
       "       [ 0.07736993,  0.        ,  0.        ],\n",
       "       [-0.07530284,  0.        ,  0.        ],\n",
       "       [-0.12230786,  0.        ,  0.        ],\n",
       "       [ 0.02849889,  0.        ,  0.        ],\n",
       "       [-0.00774491,  0.        ,  0.        ],\n",
       "       [-0.05665879,  0.        ,  0.        ],\n",
       "       [ 0.06288375,  0.        ,  0.        ],\n",
       "       [-0.06255273,  0.        ,  0.        ],\n",
       "       [-0.06228759,  0.        ,  0.        ],\n",
       "       [ 0.05354842,  0.        ,  0.        ],\n",
       "       [-0.12861589,  0.        ,  0.        ],\n",
       "       [ 0.07011004,  0.        ,  0.        ],\n",
       "       [ 0.0084565 ,  0.        ,  0.        ],\n",
       "       [-0.06337497,  0.        ,  0.        ],\n",
       "       [ 0.08542667,  0.        ,  0.        ],\n",
       "       [ 0.0402491 ,  0.        ,  0.        ],\n",
       "       [-0.00997815,  0.        ,  0.        ],\n",
       "       [ 0.09181197,  0.        ,  0.        ],\n",
       "       [ 0.03050959,  0.        ,  0.        ],\n",
       "       [-0.0414311 ,  0.        ,  0.        ],\n",
       "       [ 0.10374742,  0.        ,  0.        ],\n",
       "       [-0.09075644,  0.        ,  0.        ],\n",
       "       [ 0.0177986 ,  0.        ,  0.        ],\n",
       "       [-0.02099813,  0.        ,  0.        ],\n",
       "       [-0.09176203,  0.        ,  0.        ],\n",
       "       [ 0.06771989,  0.        ,  0.        ],\n",
       "       [-0.0856353 ,  0.        ,  0.        ],\n",
       "       [ 0.04464789,  0.        ,  0.        ],\n",
       "       [ 0.02147137,  0.        ,  0.        ],\n",
       "       [-0.03356819,  0.        ,  0.        ],\n",
       "       [ 0.01979267,  0.        ,  0.        ],\n",
       "       [ 0.02793415,  0.        ,  0.        ],\n",
       "       [-0.1130053 ,  0.        ,  0.        ],\n",
       "       [ 0.09175822,  0.        ,  0.        ],\n",
       "       [-0.0429931 ,  0.        ,  0.        ],\n",
       "       [-0.01240433,  0.        ,  0.        ],\n",
       "       [ 0.01583097,  0.        ,  0.        ],\n",
       "       [ 0.12986225,  0.        ,  0.        ],\n",
       "       [-0.01263917,  0.        ,  0.        ],\n",
       "       [ 0.15278601,  0.        ,  0.        ],\n",
       "       [-0.04894353,  0.        ,  0.        ],\n",
       "       [-0.02103683,  0.        ,  0.        ],\n",
       "       [ 0.06416374,  0.        ,  0.        ],\n",
       "       [-0.11463874,  0.        ,  0.        ],\n",
       "       [ 0.09017787,  0.        ,  0.        ],\n",
       "       [-0.10845959,  0.        ,  0.        ],\n",
       "       [ 0.03812028,  0.        ,  0.        ],\n",
       "       [-0.00471028,  0.        ,  0.        ],\n",
       "       [ 0.02283544,  0.        ,  0.        ],\n",
       "       [ 0.03792071,  0.        ,  0.        ],\n",
       "       [-0.0643574 ,  0.        ,  0.        ],\n",
       "       [ 0.03175007,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_dic[sub]['RE'][run][:,18:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "terminal-netherlands",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 2.52545e-05, -6.92254e-02,  2.07419e-02, ...,  0.00000e+00,\n",
       "          0.00000e+00,  0.00000e+00],\n",
       "        [-2.00212e-03,  4.94674e-02,  4.64707e-02, ...,  0.00000e+00,\n",
       "          0.00000e+00,  0.00000e+00],\n",
       "        [ 3.83604e-05, -6.24450e-02,  4.66857e-03, ...,  0.00000e+00,\n",
       "          0.00000e+00,  0.00000e+00],\n",
       "        ...,\n",
       "        [ 2.58818e-05, -5.82623e-02,  2.86826e-01, ...,  0.00000e+00,\n",
       "          0.00000e+00,  0.00000e+00],\n",
       "        [-7.63545e-05,  1.39179e-02,  3.22507e-01, ...,  0.00000e+00,\n",
       "          0.00000e+00,  1.00000e+00],\n",
       "        [-2.34130e-04, -6.79895e-02,  2.90948e-01, ...,  0.00000e+00,\n",
       "          0.00000e+00,  0.00000e+00]]),\n",
       " array([[ 1.66354e-06, -3.81098e-02,  3.56339e-04, ...,  0.00000e+00,\n",
       "          0.00000e+00,  0.00000e+00],\n",
       "        [-1.11097e-06, -3.04837e-02,  3.36234e-04, ...,  0.00000e+00,\n",
       "          0.00000e+00,  0.00000e+00],\n",
       "        [ 0.00000e+00, -1.48734e-02,  3.14277e-04, ...,  0.00000e+00,\n",
       "          0.00000e+00,  0.00000e+00],\n",
       "        ...,\n",
       "        [ 5.26954e-04,  3.24583e-02,  3.20048e-02, ...,  0.00000e+00,\n",
       "          0.00000e+00,  0.00000e+00],\n",
       "        [ 5.75478e-04,  1.11653e-01,  1.30844e-01, ...,  0.00000e+00,\n",
       "          0.00000e+00,  0.00000e+00],\n",
       "        [-6.31450e-03,  5.46331e-02,  4.34147e-02, ...,  0.00000e+00,\n",
       "          0.00000e+00,  0.00000e+00]])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_dic['sub-005']['OM']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-selection",
   "metadata": {},
   "source": [
    "# Exclude runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How to use this part:\n",
    "- not quite sure how the hell i created run_dic, but consists of all the runs to be included \n",
    "    based on which subjects had button presses: np.array([0,1]) if both runs to be included \n",
    "- conf_sub = full confound list to be condensed based on run exclusion\n",
    "- n28_numtrs_censored_at_threshp3.npy: created using excel part below which sums TRs with more than .x head motion\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "centered-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change me ! and the files to load in below## \n",
    "out_name = 'n28_conf+cens_MERGE_removNoBpress_delHMruns_threshp5_glm.npy'\n",
    "# ** may have to change this! could be 205 if some trs are on the borderline\n",
    "abov_thresh = int(209*.3)\n",
    "cond_list = ['SM', \"SC\", \"RE\", 'OM', 'OC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "collected-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Confounds #\n",
    "conf_sub = dict(enumerate(np.load(os.path.join(confounds_dir, 'n28_conf+cens_MERGE_10r_threshp5_glm.npy'), \n",
    "                                          allow_pickle = True).flatten(),1))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "single-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load total TRs censored #\n",
    "tot_cen = dict(enumerate(np.load(os.path.join(confounds_dir, 'n28_numtrs_censored_at_threshp5.npy'), \n",
    "                                          allow_pickle = True).flatten(),1))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "narrow-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "## runs to include -- this was manually created, don't change ## \n",
    "run_dic = dict(enumerate(np.load(os.path.join(load_bpress, \"n28_runs_2_include.npy\"), \n",
    "                                allow_pickle=True).flatten(),1))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "affiliated-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 1 ## \n",
    "# This loop will iterate through tot cens, which is a dictionary defining how many TRs are above a thresh\n",
    "# produces a cens_dat matrix consiting of which runs to include\n",
    "cens_dat = {}\n",
    "for sub in sub_list:\n",
    "    temp = {}\n",
    "    for cond in cond_list:\n",
    "        temp2= []\n",
    "        for run_num in range(2):\n",
    "            if tot_cen[sub][cond+\"-\"+str(run_num+1)] < abov_thresh:\n",
    "                temp2.append(run_num)\n",
    "        # save runs where the indices should be included according to run_dic\n",
    "        temp[cond] = np.asarray(temp2)\n",
    "    cens_dat[sub] = temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-balloon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "strong-optimization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-000\n",
      "sub-001\n",
      "sub-002\n",
      "sub-003\n",
      "sub-004\n",
      "sub-005\n",
      "sub-006\n",
      "sub-007\n",
      "sub-008\n",
      "sub-009\n",
      "sub-010\n",
      "sub-011\n",
      "sub-012\n",
      "sub-013\n",
      "sub-014\n",
      "sub-015\n",
      "sub-016\n",
      "sub-017\n",
      "sub-018\n",
      "sub-019\n",
      "sub-020\n",
      "sub-021\n",
      "sub-022\n",
      "sub-023\n",
      "sub-024\n",
      "sub-025\n",
      "sub-026\n",
      "sub-027\n"
     ]
    }
   ],
   "source": [
    "## Part 2 ## \n",
    "# Remove runs without a button press AND runs above the censor threshold#\n",
    "# produces confound matrix with no bpress runs removed and censor threshold removed\n",
    "\n",
    "conf_sub2 = {}\n",
    "for sub in sub_list:\n",
    "    print(sub)\n",
    "    temp = {}\n",
    "    for cond in cond_list:\n",
    "        # select only the runs where there was a button press and above the mot thresh\n",
    "        intersect_arr = np.intersect1d(run_dic[sub][cond], cens_dat[sub][cond])\n",
    "    \n",
    "        # save runs where the indices should be included according to run_dic\n",
    "        temp[cond] = [conf_sub[sub][cond][i] for i in intersect_arr]\n",
    "        #print(conf_sub[sub][cond])\n",
    "    conf_sub2[sub] = temp\n",
    "\n",
    "#np.save(os.path.join(confounds_dir, out_name),conf_sub2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "natural-alias",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(np.array([0,1]), np.array([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-victoria",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-queensland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-egypt",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-lexington",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-friendly",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is what i used for the excel doc, not for head motion censoring for mvpa, which is above and unfinished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-newsletter",
   "metadata": {},
   "source": [
    "# Excel output by condition hm - Average head motion across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "academic-living",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "36\n",
      "45\n",
      "23\n",
      "5\n",
      "9\n",
      "34\n",
      "43\n",
      "16\n",
      "12\n",
      "3\n",
      "9\n",
      "10\n",
      "31\n",
      "23\n",
      "9\n",
      "34\n",
      "40\n",
      "29\n",
      "12\n",
      "9\n",
      "61\n",
      "32\n",
      "19\n",
      "7\n",
      "36\n",
      "12\n",
      "14\n",
      "82\n",
      "11\n",
      "8\n",
      "6\n",
      "32\n",
      "34\n",
      "13\n",
      "30\n",
      "31\n",
      "46\n",
      "33\n",
      "49\n",
      "5\n",
      "0\n",
      "10\n",
      "14\n",
      "19\n",
      "17\n",
      "23\n",
      "8\n",
      "18\n",
      "6\n",
      "0\n",
      "4\n",
      "80\n",
      "38\n",
      "92\n",
      "9\n",
      "56\n",
      "134\n",
      "116\n",
      "8\n",
      "2\n",
      "6\n",
      "10\n",
      "66\n",
      "56\n",
      "6\n",
      "51\n",
      "77\n",
      "18\n",
      "14\n",
      "0\n",
      "19\n",
      "17\n",
      "38\n",
      "20\n",
      "89\n",
      "85\n",
      "19\n",
      "86\n",
      "1\n",
      "0\n",
      "1\n",
      "47\n",
      "7\n",
      "91\n",
      "2\n",
      "68\n",
      "69\n",
      "15\n",
      "0\n",
      "2\n",
      "2\n",
      "6\n",
      "7\n",
      "6\n",
      "14\n",
      "5\n",
      "0\n",
      "2\n",
      "4\n",
      "12\n",
      "6\n",
      "56\n",
      "28\n",
      "8\n",
      "95\n",
      "41\n",
      "32\n",
      "63\n",
      "10\n",
      "7\n",
      "6\n",
      "56\n",
      "67\n",
      "28\n",
      "34\n",
      "33\n",
      "29\n",
      "1\n",
      "2\n",
      "60\n",
      "59\n",
      "57\n",
      "42\n",
      "69\n",
      "46\n",
      "77\n",
      "80\n",
      "73\n",
      "64\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "10\n",
      "4\n",
      "0\n",
      "2\n",
      "5\n",
      "8\n",
      "6\n",
      "42\n",
      "2\n",
      "2\n",
      "7\n",
      "13\n",
      "8\n",
      "25\n",
      "6\n",
      "13\n",
      "41\n",
      "14\n",
      "19\n",
      "16\n",
      "27\n",
      "9\n",
      "10\n",
      "2\n",
      "7\n",
      "2\n",
      "15\n",
      "9\n",
      "3\n",
      "6\n",
      "17\n",
      "10\n",
      "7\n",
      "8\n",
      "2\n",
      "0\n",
      "0\n",
      "21\n",
      "63\n",
      "3\n",
      "33\n",
      "0\n",
      "10\n",
      "0\n",
      "4\n",
      "7\n",
      "14\n",
      "11\n",
      "5\n",
      "1\n",
      "18\n",
      "14\n",
      "6\n",
      "1\n",
      "62\n",
      "14\n",
      "29\n",
      "39\n",
      "42\n",
      "43\n",
      "67\n",
      "86\n",
      "67\n",
      "65\n",
      "0\n",
      "0\n",
      "0\n",
      "32\n",
      "8\n",
      "3\n",
      "11\n",
      "3\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "27\n",
      "8\n",
      "5\n",
      "19\n",
      "16\n",
      "14\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "################## censor me timbers ##################\n",
    "head_mot_dic = {}\n",
    "fd_arr = []\n",
    "cond_list = ['SM', \"SC\", \"RE\", 'OM', 'OC']\n",
    "all_subs = {}\n",
    "av_fd = {}\n",
    "\n",
    "for sub in sub_list:\n",
    "    temp_dic = {}\n",
    "    t_dic = {}\n",
    "    lab_indic = label_indices(behav_p, sub)\n",
    "    for run in range(1,num_runs+1):\n",
    "        if sub == \"sub-002\":\n",
    "            if run >=7:\n",
    "                run = run+1\n",
    "        confounds_fn = pd.read_csv(os.path.join(data_dir, sub + \"/ses-01/func\",\"%s_ses-01_task-Attn_run-%s_desc-confounds_timeseries.tsv\" % (sub, run)),sep='\\t')\n",
    "        print(len(confounds_fn[confounds_fn['framewise_displacement'] > fd]['framewise_displacement']))\n",
    "        # Find percentage that were censored for each run at the current threshold\n",
    "        #tot_cens = len(confounds_fn[confounds_fn['framewise_displacement'] > fd]['framewise_displacement']) /209\n",
    "        #Find average framewsie displacement for the current run\n",
    "        tot_cens = np.nanmean(np.asarray(confounds_fn['framewise_displacement']))\n",
    "        # creat binary array\n",
    "        fd_arr.append(np.asarray(confounds_fn['framewise_displacement'] > fd).astype(int))\n",
    "    #np.save all_dic[sub] fd_arr\n",
    "        if sub ==\"sub-002\" and run>=7:\n",
    "            temp_dic[run-2] = tot_cens\n",
    "        else:\n",
    "            temp_dic[run-1] = tot_cens\n",
    "            \n",
    "    # Above we aggregate the num censored into a list of 10. BELOW we SORT!\n",
    "    for cond in cond_list:\n",
    "        #print(cond)\n",
    "        run1 = temp_dic[lab_indic[cond][0]] \n",
    "        run2 = temp_dic[lab_indic[cond][1]]\n",
    "        t_dic[cond+\"-1\"] = run1\n",
    "        t_dic[cond+\"-2\"] = run2\n",
    "    all_subs[sub] = t_dic\n",
    "    \n",
    "    # Now calculate average framewise displacement across sm,sc\n",
    "    av_fd[sub] = [np.mean(np.array([t_dic['SM-1'],t_dic['SM-2'],t_dic['SC-1'],t_dic['SC-1']]))]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-notice",
   "metadata": {},
   "source": [
    "# Excel Find num TRs censored via sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "timely-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "jewish-leonard",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "4\n",
      "1\n",
      "1\n",
      "4\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "15\n",
      "0\n",
      "4\n",
      "1\n",
      "8\n",
      "1\n",
      "6\n",
      "24\n",
      "0\n",
      "2\n",
      "0\n",
      "16\n",
      "15\n",
      "4\n",
      "17\n",
      "12\n",
      "24\n",
      "12\n",
      "17\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "5\n",
      "1\n",
      "3\n",
      "4\n",
      "0\n",
      "0\n",
      "12\n",
      "3\n",
      "21\n",
      "0\n",
      "8\n",
      "51\n",
      "22\n",
      "0\n",
      "0\n",
      "2\n",
      "4\n",
      "17\n",
      "26\n",
      "3\n",
      "19\n",
      "22\n",
      "7\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "7\n",
      "45\n",
      "26\n",
      "0\n",
      "10\n",
      "0\n",
      "0\n",
      "0\n",
      "15\n",
      "1\n",
      "37\n",
      "0\n",
      "25\n",
      "25\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "17\n",
      "1\n",
      "4\n",
      "16\n",
      "7\n",
      "1\n",
      "20\n",
      "2\n",
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "11\n",
      "3\n",
      "10\n",
      "13\n",
      "24\n",
      "4\n",
      "9\n",
      "18\n",
      "24\n",
      "17\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "15\n",
      "0\n",
      "0\n",
      "2\n",
      "4\n",
      "1\n",
      "6\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "7\n",
      "1\n",
      "3\n",
      "3\n",
      "2\n",
      "16\n",
      "11\n",
      "41\n",
      "9\n",
      "32\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "################## censor me timbers ##################\n",
    "head_mot_dic = {}\n",
    "fd_arr = []\n",
    "cond_list = ['SM', \"SC\", \"RE\", 'OM', 'OC']\n",
    "all_subs = {}\n",
    "av_fd = {}\n",
    "\n",
    "for sub in sub_list:\n",
    "    temp_dic = {}\n",
    "    t_dic = {}\n",
    "    lab_indic = label_indices(behav_p, sub)\n",
    "    for run in range(1,num_runs+1):\n",
    "        if sub == \"sub-002\":\n",
    "            if run >=7:\n",
    "                run = run+1\n",
    "        confounds_fn = pd.read_csv(os.path.join(data_dir, sub + \"/ses-01/func\",\"%s_ses-01_task-Attn_run-%s_desc-confounds_timeseries.tsv\" % (sub, run)),sep='\\t')\n",
    "        print(len(confounds_fn[confounds_fn['framewise_displacement'] > fd]['framewise_displacement']))\n",
    "        # Find percentage that were censored for each run at the current threshold\n",
    "        #tot_cens = len(confounds_fn[confounds_fn['framewise_displacement'] > fd]['framewise_displacement']) /209\n",
    "        #Find average framewsie displacement for the current run\n",
    "        tot_cens = np.nanmean(np.asarray(confounds_fn['framewise_displacement']))\n",
    "        # creat binary array\n",
    "        fd_arr.append(np.asarray(confounds_fn['framewise_displacement'] > fd).astype(int))\n",
    "    #np.save all_dic[sub] fd_arr\n",
    "        if sub ==\"sub-002\" and run>=7:\n",
    "            temp_dic[run-2] = np.sum(np.asarray(confounds_fn['framewise_displacement'] > fd).astype(int))\n",
    "        else:\n",
    "            temp_dic[run-1] = np.sum(np.asarray(confounds_fn['framewise_displacement'] > fd).astype(int))\n",
    "            \n",
    "    # Above we aggregate the num censored into a list of 10. BELOW we SORT!\n",
    "    for cond in cond_list:\n",
    "        #print(cond)\n",
    "        run1 = temp_dic[lab_indic[cond][0]] \n",
    "        run2 = temp_dic[lab_indic[cond][1]]\n",
    "        t_dic[cond+\"-1\"] = run1\n",
    "        t_dic[cond+\"-2\"] = run2\n",
    "    all_subs[sub] = t_dic\n",
    "    \n",
    "    # Now calculate average framewise displacement across sm,sc\n",
    "    #av_fd[sub] = [np.mean(np.array([t_dic['SM-1'],t_dic['SM-2'],t_dic['SC-1'],t_dic['SC-1']]))]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "rational-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(os.path.join(confounds_dir, 'n28_numtrs_censored_at_threshp5.npy'),all_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "romance-tongue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SM-1': 36,\n",
       " 'SM-2': 16,\n",
       " 'SC-1': 45,\n",
       " 'SC-2': 43,\n",
       " 'RE-1': 27,\n",
       " 'RE-2': 12,\n",
       " 'OM-1': 5,\n",
       " 'OM-2': 9,\n",
       " 'OC-1': 23,\n",
       " 'OC-2': 34}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subs['sub-000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "corrected-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_sum = pd.DataFrame(all_subs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-spring",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "promotional-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_sum.to_csv('n28_hm_tot_p4.csv',index=False,sep='\\t',mode='a')\n",
    "!mv n28_hm_tot_p4.csv /jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/censor_hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rsync -rvp isaacrc@scotty.princeton.edu:/jukebox/graziano/coolCatIsaac/ATM/data/behavioral/n2* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cathedral-commander",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SM-1</th>\n",
       "      <th>SM-2</th>\n",
       "      <th>SC-1</th>\n",
       "      <th>SC-2</th>\n",
       "      <th>RE-1</th>\n",
       "      <th>RE-2</th>\n",
       "      <th>OM-1</th>\n",
       "      <th>OM-2</th>\n",
       "      <th>OC-1</th>\n",
       "      <th>OC-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sub-000</th>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-001</th>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-002</th>\n",
       "      <td>61</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-003</th>\n",
       "      <td>32</td>\n",
       "      <td>46</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>49</td>\n",
       "      <td>34</td>\n",
       "      <td>31</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-004</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-005</th>\n",
       "      <td>92</td>\n",
       "      <td>134</td>\n",
       "      <td>80</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-006</th>\n",
       "      <td>6</td>\n",
       "      <td>77</td>\n",
       "      <td>66</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>56</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-007</th>\n",
       "      <td>19</td>\n",
       "      <td>89</td>\n",
       "      <td>17</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>38</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-008</th>\n",
       "      <td>91</td>\n",
       "      <td>68</td>\n",
       "      <td>47</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-009</th>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-010</th>\n",
       "      <td>28</td>\n",
       "      <td>95</td>\n",
       "      <td>56</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>63</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-011</th>\n",
       "      <td>67</td>\n",
       "      <td>29</td>\n",
       "      <td>56</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-012</th>\n",
       "      <td>57</td>\n",
       "      <td>46</td>\n",
       "      <td>59</td>\n",
       "      <td>77</td>\n",
       "      <td>60</td>\n",
       "      <td>64</td>\n",
       "      <td>69</td>\n",
       "      <td>80</td>\n",
       "      <td>42</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-013</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-014</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-015</th>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-016</th>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-017</th>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>63</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-018</th>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-019</th>\n",
       "      <td>14</td>\n",
       "      <td>67</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>62</td>\n",
       "      <td>65</td>\n",
       "      <td>39</td>\n",
       "      <td>67</td>\n",
       "      <td>29</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-020</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-021</th>\n",
       "      <td>27</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-022</th>\n",
       "      <td>21</td>\n",
       "      <td>26</td>\n",
       "      <td>67</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-023</th>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-024</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-025</th>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-026</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-027</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SM-1  SM-2  SC-1  SC-2  RE-1  RE-2  OM-1  OM-2  OC-1  OC-2\n",
       "sub-000    36    16    45    43    27    12     5     9    23    34\n",
       "sub-001     9    29    31    40     3    12    10     9    23    34\n",
       "sub-002    61    82    32    36     9    11    19    14     7    12\n",
       "sub-003    32    46     6    30     8    49    34    31    13    33\n",
       "sub-004     0     8    19    23     5     6    10    17    14    18\n",
       "sub-005    92   134    80   116     0     8     4     9    38    56\n",
       "sub-006     6    77    66    51     2    14    56    18    10     6\n",
       "sub-007    19    89    17    85     0     1    20    19    38    86\n",
       "sub-008    91    68    47    69     0     0     7    15     1     2\n",
       "sub-009     6    14     7     2     2     4     2     0     6     5\n",
       "sub-010    28    95    56    32    12    10     8    63     6    41\n",
       "sub-011    67    29    56    34     7     2     6     1    28    33\n",
       "sub-012    57    46    59    77    60    64    69    80    42    73\n",
       "sub-013     0     0     0     4     0     2     3     1     0    10\n",
       "sub-014     8     8    42     7     5    25     2    13     6     2\n",
       "sub-015    14    10    41    27     6     2    19     9    13    16\n",
       "sub-016    15     6     9    17     7     8     3     7     2    10\n",
       "sub-017    21    10    63    33     2     0     0     3     0     0\n",
       "sub-018    11    18    14    14     4     1     7     6     5     1\n",
       "sub-019    14    67    42    43    62    65    39    67    29    86\n",
       "sub-020     8    11    32     3     0     2     0     3     0     4\n",
       "sub-021    27    16     8    19     0     4     2     5     3    14\n",
       "sub-022    21    26    67    38     1     0     0     0     0     0\n",
       "sub-023     9    26     9    33     0     6     0     4     1     0\n",
       "sub-024     0     1     1     0     3     0     0     0     1     0\n",
       "sub-025     4    19     1     2     1     0     1     6     1     5\n",
       "sub-026     1     4     3     3    34     0     2     2     2     3\n",
       "sub-027     0     2     0     0     0     0     0     0     0     1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-david",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-account",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-reading",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "imported-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "covar_hm_tab = pd.DataFrame(av_fd).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "false-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "covar_hm_tab = covar_hm_tab.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "european-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "covar_hm_tab.columns = ['subject','hm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "balanced-lewis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>hm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-000</td>\n",
       "      <td>0.201538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-001</td>\n",
       "      <td>0.192238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-002</td>\n",
       "      <td>0.241902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-003</td>\n",
       "      <td>0.207807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-004</td>\n",
       "      <td>0.155668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sub-005</td>\n",
       "      <td>0.309290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sub-006</td>\n",
       "      <td>0.247485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sub-007</td>\n",
       "      <td>0.229260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sub-008</td>\n",
       "      <td>0.250083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sub-009</td>\n",
       "      <td>0.149835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sub-010</td>\n",
       "      <td>0.259786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sub-011</td>\n",
       "      <td>0.226328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sub-012</td>\n",
       "      <td>0.244122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sub-013</td>\n",
       "      <td>0.117108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sub-014</td>\n",
       "      <td>0.195018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sub-015</td>\n",
       "      <td>0.222459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sub-016</td>\n",
       "      <td>0.135445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sub-017</td>\n",
       "      <td>0.215880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sub-018</td>\n",
       "      <td>0.166480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sub-019</td>\n",
       "      <td>0.210515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sub-020</td>\n",
       "      <td>0.178002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sub-021</td>\n",
       "      <td>0.177627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject        hm\n",
       "0   sub-000  0.201538\n",
       "1   sub-001  0.192238\n",
       "2   sub-002  0.241902\n",
       "3   sub-003  0.207807\n",
       "4   sub-004  0.155668\n",
       "5   sub-005  0.309290\n",
       "6   sub-006  0.247485\n",
       "7   sub-007  0.229260\n",
       "8   sub-008  0.250083\n",
       "9   sub-009  0.149835\n",
       "10  sub-010  0.259786\n",
       "11  sub-011  0.226328\n",
       "12  sub-012  0.244122\n",
       "13  sub-013  0.117108\n",
       "14  sub-014  0.195018\n",
       "15  sub-015  0.222459\n",
       "16  sub-016  0.135445\n",
       "17  sub-017  0.215880\n",
       "18  sub-018  0.166480\n",
       "19  sub-019  0.210515\n",
       "20  sub-020  0.178002\n",
       "21  sub-021  0.177627"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covar_hm_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ecological-fantasy",
   "metadata": {},
   "outputs": [],
   "source": [
    "covar_hm_tab.to_csv('covar_hm_tab.txt',index=False,sep='\\t',mode='a')\n",
    "!mv covar_hm_tab.txt /jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/censor_hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "configured-making",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "abandoned-training",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/censor_hm'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "censor_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-stock",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-parts",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-pillow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "otherwise-giant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OC-1</th>\n",
       "      <th>OC-2</th>\n",
       "      <th>OM-1</th>\n",
       "      <th>OM-2</th>\n",
       "      <th>RE-1</th>\n",
       "      <th>RE-2</th>\n",
       "      <th>SC-1</th>\n",
       "      <th>SC-2</th>\n",
       "      <th>SM-1</th>\n",
       "      <th>SM-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sub-003</th>\n",
       "      <td>0.16136</td>\n",
       "      <td>0.23059</td>\n",
       "      <td>0.239314</td>\n",
       "      <td>0.246183</td>\n",
       "      <td>0.156486</td>\n",
       "      <td>0.280248</td>\n",
       "      <td>0.159266</td>\n",
       "      <td>0.236233</td>\n",
       "      <td>0.237851</td>\n",
       "      <td>0.274843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            OC-1     OC-2      OM-1      OM-2      RE-1      RE-2      SC-1  \\\n",
       "sub-003  0.16136  0.23059  0.239314  0.246183  0.156486  0.280248  0.159266   \n",
       "\n",
       "             SC-2      SM-1      SM-2  \n",
       "sub-003  0.236233  0.237851  0.274843  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_subs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "elect-bangkok",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SM-1</th>\n",
       "      <th>SM-2</th>\n",
       "      <th>SC-1</th>\n",
       "      <th>SC-2</th>\n",
       "      <th>RE-1</th>\n",
       "      <th>RE-2</th>\n",
       "      <th>OM-1</th>\n",
       "      <th>OM-2</th>\n",
       "      <th>OC-1</th>\n",
       "      <th>OC-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sub-000</th>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-001</th>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-002</th>\n",
       "      <td>61</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-003</th>\n",
       "      <td>32</td>\n",
       "      <td>46</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>49</td>\n",
       "      <td>34</td>\n",
       "      <td>31</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-004</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-005</th>\n",
       "      <td>92</td>\n",
       "      <td>134</td>\n",
       "      <td>80</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-006</th>\n",
       "      <td>6</td>\n",
       "      <td>77</td>\n",
       "      <td>66</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>56</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-007</th>\n",
       "      <td>19</td>\n",
       "      <td>89</td>\n",
       "      <td>17</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>38</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-008</th>\n",
       "      <td>91</td>\n",
       "      <td>68</td>\n",
       "      <td>47</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-009</th>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-010</th>\n",
       "      <td>28</td>\n",
       "      <td>95</td>\n",
       "      <td>56</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>63</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-011</th>\n",
       "      <td>67</td>\n",
       "      <td>29</td>\n",
       "      <td>56</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-012</th>\n",
       "      <td>57</td>\n",
       "      <td>46</td>\n",
       "      <td>59</td>\n",
       "      <td>77</td>\n",
       "      <td>60</td>\n",
       "      <td>64</td>\n",
       "      <td>69</td>\n",
       "      <td>80</td>\n",
       "      <td>42</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-013</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-014</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-015</th>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-016</th>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-017</th>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>63</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-018</th>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-019</th>\n",
       "      <td>14</td>\n",
       "      <td>67</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>62</td>\n",
       "      <td>65</td>\n",
       "      <td>39</td>\n",
       "      <td>67</td>\n",
       "      <td>29</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-020</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-021</th>\n",
       "      <td>27</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SM-1  SM-2  SC-1  SC-2  RE-1  RE-2  OM-1  OM-2  OC-1  OC-2\n",
       "sub-000    36    16    45    43    27    12     5     9    23    34\n",
       "sub-001     9    29    31    40     3    12    10     9    23    34\n",
       "sub-002    61    82    32    36     9    11    19    14     7    12\n",
       "sub-003    32    46     6    30     8    49    34    31    13    33\n",
       "sub-004     0     8    19    23     5     6    10    17    14    18\n",
       "sub-005    92   134    80   116     0     8     4     9    38    56\n",
       "sub-006     6    77    66    51     2    14    56    18    10     6\n",
       "sub-007    19    89    17    85     0     1    20    19    38    86\n",
       "sub-008    91    68    47    69     0     0     7    15     1     2\n",
       "sub-009     6    14     7     2     2     4     2     0     6     5\n",
       "sub-010    28    95    56    32    12    10     8    63     6    41\n",
       "sub-011    67    29    56    34     7     2     6     1    28    33\n",
       "sub-012    57    46    59    77    60    64    69    80    42    73\n",
       "sub-013     0     0     0     4     0     2     3     1     0    10\n",
       "sub-014     8     8    42     7     5    25     2    13     6     2\n",
       "sub-015    14    10    41    27     6     2    19     9    13    16\n",
       "sub-016    15     6     9    17     7     8     3     7     2    10\n",
       "sub-017    21    10    63    33     2     0     0     3     0     0\n",
       "sub-018    11    18    14    14     4     1     7     6     5     1\n",
       "sub-019    14    67    42    43    62    65    39    67    29    86\n",
       "sub-020     8    11    32     3     0     2     0     3     0     4\n",
       "sub-021    27    16     8    19     0     4     2     5     3    14"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_subs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "organizational-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_subs).T.to_csv('head_mot_sum.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-partnership",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-tuning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-computer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "individual-apple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n",
      "(92067,)\n"
     ]
    }
   ],
   "source": [
    "for idx, dat in enumerate(maskedData):\n",
    "    print(dat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "arbitrary-chapter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(maskedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "potential-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(mot_run, bold_run):\n",
    "    censored_dat = []\n",
    "    for idx in range(len(bold_run)):\n",
    "        # if index does not equal censored, add it to the cleaned dataframe\n",
    "        if mot_run[idx] != 1:\n",
    "            censored_dat.append(bold_run[idx,:])\n",
    "    print(len(censored_dat))\n",
    "    return np.asarray(censored_dat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "improving-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list = ['sub-000']\n",
    "sub = 'sub-000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "common-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_mot = list(np.load(os.path.join(censor_dir + \"/%s_censor_10r.npy\" % (sub))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "applied-november",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_mot[0][9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "stock-motel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "200 - np.sum(head_mot[0][9:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "strange-training",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/load_fcma/mask_10r_n22-subs.nii.gz'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "normal-chrome",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2.5,    0. ,    0. ,  -96.5],\n",
       "       [   0. ,    2.5,    0. , -132.5],\n",
       "       [   0. ,    0. ,    3. ,  -78.5],\n",
       "       [   0. ,    0. ,    0. ,    1. ]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats[0].affine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "classical-setting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POW, right in the kisser! Begin sub-000\n",
      "(200, 92067)\n",
      "173\n",
      "(173, 92067)\n"
     ]
    },
    {
     "ename": "HeaderDataError",
     "evalue": "shape (173, 92067) does not fit in dim datatype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHeaderDataError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-79c40adffa76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mcleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_mot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaskedData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNifti1Image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mcleaned_runs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_runs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nibabel/nifti1.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataobj, affine, header, extra, file_map)\u001b[0m\n\u001b[1;32m   1758\u001b[0m                                          \u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m                                          \u001b[0mextra\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1760\u001b[0;31m                                          file_map)\n\u001b[0m\u001b[1;32m   1761\u001b[0m         \u001b[0;31m# Force set of s/q form when header is None unless affine is also None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maffine\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nibabel/analyze.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataobj, affine, header, extra, file_map)\u001b[0m\n\u001b[1;32m    917\u001b[0m                  extra=None, file_map=None):\n\u001b[1;32m    918\u001b[0m         super(AnalyzeImage, self).__init__(\n\u001b[0;32m--> 919\u001b[0;31m             dataobj, affine, header, extra, file_map)\n\u001b[0m\u001b[1;32m    920\u001b[0m         \u001b[0;31m# Reset consumable values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_header\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data_offset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nibabel/spatialimages.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataobj, affine, header, extra, file_map)\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_header\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# make header correspond with image and affine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nibabel/nifti1.py\u001b[0m in \u001b[0;36mupdate_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2030\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2031\u001b[0m         \u001b[0;34m\"\"\" Harmonize header with image data and affine \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2032\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNifti1Image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2033\u001b[0m         \u001b[0mhdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2034\u001b[0m         \u001b[0mhdr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'magic'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_magic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nibabel/nifti1.py\u001b[0m in \u001b[0;36mupdate_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1793\u001b[0m         \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m         \"\"\"\n\u001b[0;32m-> 1795\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNifti1Pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1796\u001b[0m         \u001b[0mhdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m         \u001b[0mhdr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'magic'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpair_magic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nibabel/spatialimages.py\u001b[0m in \u001b[0;36mupdate_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;31m# maybe it happened\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mhdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0;31m# If the affine is not None, and it is different from the main affine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# in the header, update the header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nibabel/nifti1.py\u001b[0m in \u001b[0;36mset_data_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    878\u001b[0m                           'not be compatible with SPM or FSL', stacklevel=2)\n\u001b[1;32m    879\u001b[0m             \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNifti1Header\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_qform_quaternion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nibabel/analyze.py\u001b[0m in \u001b[0;36mset_data_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;31m# Error if we did not succeed setting dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalues_fit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHeaderDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'shape {shape} does not fit in dim datatype'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pixdim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHeaderDataError\u001b[0m: shape (173, 92067) does not fit in dim datatype"
     ]
    }
   ],
   "source": [
    "mask_file = os.path.join(sav_fcma, 'mask_10r_n22-subs.nii.gz')\n",
    "\n",
    "# LOAD group mask\n",
    "for sub in sub_list:\n",
    "    print(\"POW, right in the kisser! Begin\", sub)\n",
    "    cleaned_runs = []\n",
    "    cats = list(np.load(load_work + sub + \"_\" + 'fwm7_mni_NOnorm' + \".npy\", allow_pickle = True))\n",
    "    head_mot = list(np.load(os.path.join(censor_dir + \"/%s_censor_10r.npy\" % (sub))))\n",
    "    for run in range(num_runs):\n",
    "        # Mask data\n",
    "        nifti_masker = NiftiMasker(mask_img=mask_file)\n",
    "        maskedData = nifti_masker.fit_transform(cats[run])\n",
    "        print(maskedData.shape)\n",
    "        run_mot = head_mot[run][9:]\n",
    "        cleaned = clean(run_mot, maskedData)\n",
    "        print(cleaned.shape)\n",
    "        img = nib.Nifti1Image(cleaned, cats[0].affine, cats[0].header)\n",
    "        cleaned_runs.append(img)\n",
    "    print(cleaned_runs)\n",
    "    #np.save(sav_work + sub + \"_\"+ suffix + \".npy\", concat, allow_pickle = True)\n",
    "print(\"phew, finished. go grab a cup of tea\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "middle-carter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92067,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maskedData[1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "magnetic-light",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'affine'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-73b00f646f26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNifti1Image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'affine'"
     ]
    }
   ],
   "source": [
    "img = nib.Nifti1Image(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats[0].affine, cats[0].header"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pointed-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python resample.py $IMAGE_TO_RESAMPLE $REFERENCE_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "french-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter nbconvert --to python slurm_create-data_preproc_ISC.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eleven-wedding",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nilearn/datasets/__init__.py:90: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "from nilearn.input_data import NiftiMasker , MultiNiftiMasker\n",
    "import nilearn as nil\n",
    "import numpy as np \n",
    "import os\n",
    "import os.path\n",
    "import scipy.io\n",
    "import nibabel as nib\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.masking import compute_epi_mask\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from copy import deepcopy\n",
    "\n",
    "import warnings\n",
    "import sys  \n",
    "import random\n",
    "# import logging\n",
    "\n",
    "import deepdish as dd\n",
    "import numpy as np\n",
    "\n",
    "import brainiak.eventseg.event\n",
    "import nibabel as nib\n",
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, zscore, pearsonr\n",
    "from scipy.signal import gaussian, convolve\n",
    "from sklearn import decomposition\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns \n",
    "\n",
    "#%matplotlib inline\n",
    "from brainiak import image, io\n",
    "from scipy.stats import stats\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from brainiak import image, io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Import machine learning libraries\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold, f_classif, SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import sem\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import statistics\n",
    "\n",
    "# Visualize it as an ROI\n",
    "from nilearn.plotting import plot_roi\n",
    "#plot_roi(x)\n",
    "\n",
    "\n",
    "from nilearn.image import concat_imgs, resample_img, mean_img\n",
    "from nilearn.plotting import view_img\n",
    "\n",
    "\n",
    "from nilearn import datasets, plotting\n",
    "from nilearn.input_data import NiftiSpheresMasker\n",
    "\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "from nilearn.image import concat_imgs, resample_img, mean_img,index_img\n",
    "from nilearn import image\n",
    "from nilearn import masking\n",
    "from nilearn.plotting import view_img\n",
    "from nilearn.image import resample_to_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mysterious-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import smooth_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-scope",
   "metadata": {},
   "source": [
    "# Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "recognized-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_lists(label, num_tr):\n",
    "    b = [[]]\n",
    "    a = []\n",
    "    for i in label:\n",
    "        # substring label in psychopy output\n",
    "        # if the first three characters == M_s, etc, then add correct indext to string\n",
    "        if i[1:4] == \"M_s\":\n",
    "            a.append(\"SM\")\n",
    "            b.append([0]*num_tr)\n",
    "        elif i[1:4] == \"C_s\":\n",
    "            a.append(\"SC\")\n",
    "            b.append([1]*num_tr)        \n",
    "        elif i[1:4] == \"M_o\":\n",
    "            a.append(\"OM\")\n",
    "            b.append([2]*num_tr)\n",
    "        elif i[1:4] == \"C_o\":\n",
    "            a.append(\"OC\")\n",
    "            b.append([3]*num_tr)     \n",
    "        else:\n",
    "            a.append(\"Re\")\n",
    "            b.append([4]*num_tr)     \n",
    "    return a, b[1:]\n",
    "\n",
    "def load_epi_data(sub, ses, task,run, space):\n",
    "  # Load MRI file\n",
    "    if space == \"MNI\":\n",
    "        epi_in = os.path.join(data_dir, sub, ses, 'func', \"%s_%s_task-%s_run-%s_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\" % (sub, ses, task,run))\n",
    "    elif space == \"T1\":\n",
    "        epi_in = os.path.join(data_dir, sub, ses, 'func', \"%s_%s_task-%s_run-%s_space-T1w_desc-preproc_bold.nii.gz\" % (sub, ses, task,run))\n",
    "    else:\n",
    "        print(\"wrong load epi input. check this function\")\n",
    "    epi_data = nib.load(epi_in)\n",
    "    print(epi_data.shape)\n",
    "    print(\"Loading data from %s\" % (epi_in))\n",
    "    return epi_data\n",
    "\n",
    "def load_roi_mask(ROI_name, space):\n",
    "    if space == \"MNI\":\n",
    "        maskdir = os.path.join(rois_dir)    \n",
    "        print(\"expected shape: 78, 93,65\")\n",
    "    elif space == \"T1\":\n",
    "        maskdir = os.path.join(rois_dir+ \"/T1\")\n",
    "        print(\"expected shape: 56, 72,53\")\n",
    "    else:\n",
    "        print(\"wrong mask input. check this function\")\n",
    "    # load the mask\n",
    "    maskfile = os.path.join(maskdir, \"%s.nii\" % (ROI_name))\n",
    "    mask = nib.load(maskfile)\n",
    "    print(\"mask shape: \", mask.shape)\n",
    "    print(\"Loaded %s mask\" % (ROI_name))\n",
    "    return mask\n",
    "def intersect_mask(sub, num_runs,reg, ses=\"ses-01\",task=\"Attn\"):\n",
    "    # This is based off of 'load_data' function in template\n",
    "    # Loads all fMRI runs into a matrix #\n",
    "    \"\"\"\n",
    "    reg = T1 or MNI registration?\n",
    "    norm_type = by Space or by Time? \n",
    "    \"\"\"\n",
    "    yoz = []\n",
    "    print(\"Begin intersecting, you sexy beast\")\n",
    "    for run in range(1, num_runs + 1):\n",
    "        if sub == \"sub-002\":\n",
    "            if run >=7:\n",
    "                run = run+1\n",
    "        # Load epi data \n",
    "        epi = load_epi_data(sub,ses,task,run,reg)\n",
    "        # Mask data\n",
    "        roi_samp = compute_epi_mask(epi) # -- whole brain\n",
    "        #roi_samp load_roi_mask(ROI_name,reg) # -- mask\n",
    "\n",
    "        nifti_masker = NiftiMasker(mask_img=roi_samp)\n",
    "        maskedData = nifti_masker.fit_transform(epi)\n",
    "        yoz.append(roi_samp)\n",
    "    #print(concatenated_data)\n",
    "    epi_data = nil.masking.intersect_masks(yoz)\n",
    "    print(\"all done wit da intersextion (lol)\")\n",
    "\n",
    "    return epi_data\n",
    "\n",
    "def fnd_indices(sub,behav_p):\n",
    "    behav = pd.read_csv(os.path.join(behav_p, '%s_behav_cleaned.csv') % (sub))\n",
    "    # Define the column in behav to be used for creating labels # \n",
    "    label = behav.iloc[:,1]\n",
    "    # Create an array of labels [1] AND the order in which runs occured [0]#\n",
    "    sub_ses_labels = label_lists(label, 200)\n",
    "    ## Find run sequence, extraction condition indexes from behav data ## \n",
    "    return find_cond_index(sub_ses_labels[0])\n",
    "\n",
    "def org_bdata(unsort_bdata, run_indexes, cond_a): \n",
    "    \"\"\"\n",
    "    organize two runs for concatenation\n",
    "    Two runs of cond_a\n",
    "    \n",
    "    \"\"\"\n",
    "    bold_data = []\n",
    "    a = [unsort_bdata[run_indexes[cond_a][0]], unsort_bdata[run_indexes[cond_a][1]]]\n",
    "    print(\"returning\", cond_a)\n",
    "    return np.asarray(a)\n",
    "\n",
    "def find_cond_index(sub_ses_labels):\n",
    "    \"\"\"\n",
    "    For the array of ordered run names (i.e.'Re', 'SM',) find the two indexes per condition\n",
    "    \"\"\" \n",
    "    lab_inx = []\n",
    "\n",
    "    a = []\n",
    "    b = []\n",
    "    c = []\n",
    "    d = []\n",
    "    e = []\n",
    "\n",
    "    for i in enumerate(sub_ses_labels):\n",
    "        if i[1] == \"SM\":\n",
    "            # append the index according to where it appeared in the array\n",
    "            a.append(i[0])\n",
    "        if i[1] == \"SC\":\n",
    "            b.append(i[0])\n",
    "        if i[1] == \"OM\":\n",
    "            c.append(i[0])\n",
    "        if i[1] == \"OC\":\n",
    "            d.append(i[0])\n",
    "\n",
    "    # Create a dictionary where each key contains the appropriate indexes\n",
    "    lab_indic = {\n",
    "        'SM' : a,\n",
    "        'SC' : b,\n",
    "        'OM' : c,\n",
    "        'OC' : d,\n",
    "        'RE' : [0,9]\n",
    "    }\n",
    "    return lab_indic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "arctic-commodity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#mask = intersect_mask('sub-000',10,'T1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "current-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nib.save(mask, os.path.join(sav_fcma,'mask_10r_sub-001_t1.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fMRI_isc(sub, num_runs, reg, norm_type, mask, ses=\"ses-01\",task=\"Attn\"):\n",
    "    # This is based off of 'load_data' function in template\n",
    "    # Loads all fMRI runs into a matrix #\n",
    "    \"\"\"\n",
    "    reg = T1 or MNI registration?\n",
    "    norm_type = by Space or by Time? - default is by space (rows)\n",
    "    \"\"\"\n",
    "    concatenated_data = []\n",
    "    \n",
    "    for run in range(1, num_runs + 1):\n",
    "        if sub == \"sub-002\":\n",
    "            if run >=7:\n",
    "                run = run+1\n",
    "        if sub != \"sub-010\":\n",
    "            # Load epi data \n",
    "            epi = load_epi_data(sub,ses,task,run,reg)\n",
    "        else:\n",
    "            # Load epi data \n",
    "            print(\"sub-10, watch out\")\n",
    "            bad_epi = load_epi_data(sub,ses,task,run,reg)\n",
    "            good_epi = load_epi_data(\"sub-001\",ses,task,run,reg)\n",
    "            epi = resample_to_img(bad_epi , good_epi, interpolation='nearest')\n",
    "        # delete first 9 TRs\n",
    "        epi = index_img(epi,slice(4,210))\n",
    "        \n",
    "        # load confounds\n",
    "        run_conf = np.asarray(pd.read_csv(os.path.join(confounds + sub + \"/func/\", \n",
    "                                                           '%s_ses-01_task-Attn_run-%s_desc-model_timeseries.csv') % (sub, run)))\n",
    "        print(epi.shape)\n",
    "        print(run_conf[4:].shape)\n",
    "        # clean image\n",
    "        # low_pass= .1, high_pass=1/128, .01 might be more normal...\n",
    "        \n",
    "        clean_bold = image.clean_img(epi, standardize = False, confounds = run_conf[4:], \n",
    "                                   t_r=1.5, high_pass = 1/128, mask_img = mask)\n",
    "        \n",
    "        #Smooth\n",
    "        clean_bold = image.smooth_img(clean_bold, fwhm=6)\n",
    "\n",
    "        #append to cat_dat\n",
    "        concatenated_data.append(clean_bold)\n",
    "    print(\"FINISHED YAY BEAST\")\n",
    "    return concatenated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-dylan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-custody",
   "metadata": {},
   "source": [
    "# Define Static VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "greek-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "data_dir = \"/jukebox/graziano/coolCatIsaac/ATM/data/bids/derivatives/fmriprep/\"\n",
    "rois_dir = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/rois/\"\n",
    "behav_p = '/jukebox/graziano/coolCatIsaac/ATM/data/behavioral'\n",
    "sav_work = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/results/corr_data/\"\n",
    "confounds = '/jukebox/graziano/coolCatIsaac/ATM/data/bids/derivatives/fmriprep/afni-head_mot/'\n",
    "workspace = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/\"\n",
    "parc_dir = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/rois/schaef_par/MNI/\"\n",
    "sav_fcma = '/jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/load_fcma'\n",
    "sav_bpress = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/results/bpress_GLM/\"\n",
    "isc_dir = '/jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/ISC/'\n",
    "load_bpress = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/results/bpress_GLM/behav\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-absorption",
   "metadata": {},
   "source": [
    "# Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dated-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old_img = nib.load(os.path.join(isc_dir, 'MNI152NLin2009cAsym_desc-brain_mask.nii.gz'))\n",
    "#old_img = nib.load(os.path.join(isc_dir, 'MNI152NLin2009cAsym_desc-brain_T1w.nii.gz'))\n",
    "#my_dat = nib.load(os.path.join(sav_fcma, 'mask_10r_n22-subs.nii.gz'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "advisory-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_img = resample_to_img(old_img , my_dat, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "angry-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "superb-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nib.save(new_img, os.path.join(isc_dir, 'MNI152NLin2009cAsym_desc-brain_T1w_RESAMP.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-driver",
   "metadata": {},
   "source": [
    "# lesss do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "consolidated-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sub_list\n",
    "sub_list = [\"sub-000\",\"sub-001\",\"sub-002\",\"sub-003\",\"sub-004\",\"sub-005\",\"sub-006\",\"sub-007\",\"sub-008\",\"sub-009\",\n",
    "            \"sub-010\",\"sub-011\",\"sub-012\",\"sub-013\",\"sub-014\",\"sub-015\", \"sub-016\",\"sub-017\", \n",
    "            \"sub-018\", \"sub-019\", \"sub-020\",\"sub-021\",\"sub-022\", \"sub-023\", \"sub-024\",\"sub-025\",\"sub-026\", \"sub-027\"]\n",
    "\n",
    "\n",
    "###### LOADING VARS #######\n",
    "# Number of runs to load \n",
    "num_runs = 10\n",
    "# Registration ust be either T1 or MNI\n",
    "reg = \"MNI\"# \"MNI\"\n",
    "# Registration Space # \n",
    "norm_type = \"space\"\n",
    "# SUFFIX - change me\n",
    "suffix = \"_205_fwhm6_hp_conf.nii\"\n",
    "\n",
    "# LOAD group mask\n",
    "mask_file = nib.load(os.path.join(isc_dir, 'MNI152NLin2009cAsym_desc-brain_mask_RESAMP.nii.gz'))\n",
    "\n",
    "# load bpress runs  for om oc # \n",
    "n_j_bpress = dict(enumerate(np.load(os.path.join(load_bpress, \"n28_omoc_n+j_runs.npy\"), \n",
    "                                allow_pickle=True).flatten(),1))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "chubby-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub_list = ['sub-000']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-approach",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "otherwise-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_list = ['SM', 'SC', 'OM',\"OC\",'RE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "understood-gossip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POW, right in the kisser! Begin sub-000\n",
      "returning SM\n",
      "returning SC\n",
      "returning OM\n",
      "returning OC\n",
      "returning RE\n",
      "phew, finished. go grab a cup of tea\n"
     ]
    }
   ],
   "source": [
    "for sub in sub_list:\n",
    "    print(\"POW, right in the kisser! Begin\", sub)\n",
    "    # Load all subject run data\n",
    "    concat = load_fMRI_isc(sub, num_runs, reg, norm_type, mask_file)\n",
    "    for cond in cond_list:\n",
    "        # find labels\n",
    "        lab_indic = fnd_indices(sub, behav_p)\n",
    "        # Organize and concatenate bold data\n",
    "        two_run_org = org_bdata(concat, lab_indic, cond)\n",
    "        if cond == \"OM\" or cond == 'OC':\n",
    "            run1 = n_j_bpress[sub][cond][0]\n",
    "            run2 = n_j_bpress[sub][cond][1]\n",
    "            nib.save(two_run_org[0], os.path.join(isc_dir,'data/'+ '%s_%s_%s_run1%s')%(sub, cond,run1, suffix))\n",
    "            nib.save(two_run_org[1], os.path.join(isc_dir,'data/'+ '%s_%s_%s_run2%s')%(sub, cond, run2, suffix))\n",
    "        else:\n",
    "            nib.save(two_run_org[0], os.path.join(isc_dir,'data/'+ '%s_%s_run1%s')%(sub, cond, suffix))\n",
    "            nib.save(two_run_org[1], os.path.join(isc_dir,'data/'+ '%s_%s_run2%s')%(sub, cond, suffix))\n",
    "\n",
    "print(\"phew, finished. go grab a cup of tea\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-recruitment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

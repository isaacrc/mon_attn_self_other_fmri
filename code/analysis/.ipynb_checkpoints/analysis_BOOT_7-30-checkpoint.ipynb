{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "available-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python resample.py $IMAGE_TO_RESAMPLE $REFERENCE_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "impressed-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "from nilearn.input_data import NiftiMasker , MultiNiftiMasker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "maritime-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn as nil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "completed-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import os.path\n",
    "import scipy.io\n",
    "import nibabel as nib\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.masking import compute_epi_mask\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "medium-lesson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import sys  \n",
    "import random\n",
    "# import logging\n",
    "\n",
    "import deepdish as dd\n",
    "import numpy as np\n",
    "\n",
    "import brainiak.eventseg.event\n",
    "import nibabel as nib\n",
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, zscore, pearsonr\n",
    "from scipy.signal import gaussian, convolve\n",
    "from sklearn import decomposition\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns \n",
    "\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\"\"\"\n",
    "from utils import sherlock_h5_data\n",
    "\n",
    "if not os.path.exists(sherlock_h5_data):\n",
    "    os.makedirs(sherlock_h5_data)\n",
    "    print('Make dir: ', sherlock_h5_data)\n",
    "else: \n",
    "    print('Data path exists')\n",
    "    \n",
    "from utils import sherlock_dir\n",
    "\"\"\"\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "usual-shell",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from brainiak import image, io\n",
    "from scipy.stats import stats\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from brainiak import image, io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "planned-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import LeavePGroupsOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "communist-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "welcome-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import machine learning libraries\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold, f_classif, SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import sem\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-amplifier",
   "metadata": {},
   "source": [
    "# Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "controversial-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expand / Label TRs\n",
    "\"\"\"\n",
    "0 = SM\n",
    "1 = SC\n",
    "2 = OM\n",
    "3 = OC\n",
    "4 = Re\n",
    "requires list of labels ouputed by psychopy (column 1 - MM_self_title.started, etc.)\n",
    "returns label list (order is preserved) and TR labels\n",
    "\"\"\"\n",
    "\n",
    "def label_lists(label, num_tr):\n",
    "    b = [[]]\n",
    "    a = []\n",
    "    for i in label:\n",
    "        # substring label in psychopy output\n",
    "        if i[1:4] == \"M_s\":\n",
    "            a.append(\"SM\")\n",
    "            b.append([0]*num_tr)\n",
    "        elif i[1:4] == \"C_s\":\n",
    "            a.append(\"SC\")\n",
    "            b.append([1]*num_tr)        \n",
    "        elif i[1:4] == \"M_o\":\n",
    "            a.append(\"OM\")\n",
    "            b.append([2]*num_tr)\n",
    "        elif i[1:4] == \"C_o\":\n",
    "            a.append(\"OC\")\n",
    "            b.append([3]*num_tr)     \n",
    "        else:\n",
    "            a.append(\"Re\")\n",
    "            b.append([4]*num_tr)     \n",
    "    return a, b[1:]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "treated-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cond_index(sub_ses_labels):\n",
    "    \"\"\"\n",
    "    For the array of ordered run names (i.e.'Re', 'SM',) find the two indexes per condition\n",
    "    \"\"\" \n",
    "    lab_inx = []\n",
    "\n",
    "    a = []\n",
    "    b = []\n",
    "    c = []\n",
    "    d = []\n",
    "\n",
    "    for i in enumerate(sub_ses_labels):\n",
    "        if i[1] == \"SM\":\n",
    "            # append the index according to where it appeared in the array\n",
    "            a.append(i[0])\n",
    "        if i[1] == \"SC\":\n",
    "            b.append(i[0])\n",
    "        if i[1] == \"OM\":\n",
    "            c.append(i[0])\n",
    "        if i[1] == \"OC\":\n",
    "            d.append(i[0])\n",
    "\n",
    "    # Create a dictionary where each key contains the appropriate indexes\n",
    "    lab_indic = {\n",
    "        'SM' : a,\n",
    "        'SC' : b,\n",
    "        'OM' : c,\n",
    "        'OC' : d\n",
    "    }\n",
    "    return lab_indic \n",
    "    #np.vstack(lab_inx, [\"SM\", \"SC\", \"OM\", \"OC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "imperial-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "def org_bold_data(cond_data, labels, run_indexes, cond_a, cond_b): \n",
    "    \"\"\"\n",
    "    Organize the bold data into an array with four runs, two conditions # \n",
    "     - Get both indexes according to the appropriate label\n",
    "     - Add cond a, then cond. Do this twice for both labels and bold data\n",
    "     - lab_indic['SM'][i] evaluates to the first, then second index for each condition\n",
    "     args:\n",
    "         cond_data: list of 8 runs of bold data\n",
    "         labels: label array for conditions\n",
    "    returns:\n",
    "        sequence of four runs of bold data, labels, for classification\n",
    "    \"\"\"\n",
    "    bold_data = []\n",
    "    label_data = []\n",
    "\n",
    "    for i in range(0,2):\n",
    "        bold_data.append(np.vstack((cond_data[run_indexes[cond_a][i]], cond_data[run_indexes[cond_b][i]])))\n",
    "        label_data.append(np.hstack((labels[run_indexes[cond_a][i]], labels[run_indexes[cond_b][i]])))\n",
    "    print(\"number of runs: \", len(bold_data), \"bold shape: \", bold_data[0].shape, \"label shape: \", label_data[0].shape)\n",
    "    print(\"Expected shape: \", num_trs *2)\n",
    "    print(\"Extracted: \", cond_a, cond_b)\n",
    "    return bold_data, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "convinced-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "####################################\n",
    "## SVM - grid search ##\n",
    "####################################\n",
    "\"\"\"\n",
    "def svm_grid_4s(bold_data, label_data,norm):\n",
    "    X = np.concatenate(bold_data)\n",
    "    y = np.concatenate(label_data)\n",
    "    len_dat = int(bold_data[0].shape[0]) + int(bold_data[1].shape[0])\n",
    "    #X = bold_data\n",
    "    #y = label_data\n",
    "    groups = np.hstack(([0]*num_trs, [1]*num_trs,[2]*num_trs,[3]*num_trs))\n",
    "    lpgo = LeavePGroupsOut(n_groups=2)\n",
    "    lpgo.get_n_splits(X, y, groups)\n",
    "    lpgo.get_n_splits(groups=groups)  # 'groups' is always required\n",
    "    #print(lpgo)\n",
    "\n",
    "    # leave out runs without two conditions (train on cond a, cond a, test on cond b, cond b)\n",
    "    # determine which runs to leave out by skipping runs that contain an index from the first\n",
    "    # run AND the third, OR an index from the second and fourth\n",
    "    # first index of each run in the entire array\n",
    "    run1_ind = int(len_dat * 0)\n",
    "    run2_ind = int(len_dat * .25)\n",
    "    run3_ind = int(len_dat * .5)\n",
    "    run4_ind = int(len_dat * .75)\n",
    "\n",
    "    clf_score = np.array([])\n",
    "    inner_clf_score = np.array([])\n",
    "    C_best = []\n",
    "\n",
    "    for train_index, test_index in lpgo.split(X, y, groups):\n",
    "        #print(groups)\n",
    "        if run1_ind in train_index and run3_ind in train_index:\n",
    "            continue\n",
    "        if run2_ind in train_index and run4_ind in train_index:\n",
    "            continue\n",
    "\n",
    "        # Train test sets # \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Normalize data \n",
    "        if norm:\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # For the present train vs test set, cross validate\n",
    "        parameters = {'C':[0.00001, 0.0001, 0.01, 0.1, 1, 10]}\n",
    "        inner_clf = GridSearchCV(\n",
    "            SVC(kernel='linear'),\n",
    "            parameters,\n",
    "            cv=5,\n",
    "            return_train_score=True)\n",
    "        inner_clf.fit(X_train, y_train)\n",
    "        #print(\"inner score: \", inner_clf.score(X_train, y_train))\n",
    "        inner_clf_score = np.hstack((inner_clf_score, inner_clf.score(X_train, y_train)))\n",
    "\n",
    "        # Find the best hyperparameter\n",
    "        C_best_i = inner_clf.best_params_['C']\n",
    "        C_best.append(C_best_i)\n",
    "\n",
    "        # Train the classifier with the best hyperparameter using training and validation set\n",
    "        classifier = SVC(kernel=\"linear\", C=C_best_i)\n",
    "        clf = classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Test the classifier\n",
    "        score = clf.score(X_test, y_test)\n",
    "        clf_score = np.hstack((clf_score, score))\n",
    "\n",
    "    #print ('Inner loop classification accuracy:', np.mean(inner_clf_score))\n",
    "    #print('best c: ', C_best_i)\n",
    "    #print ('Overall accuracy: ', np.mean(clf_score))\n",
    "    return np.mean(clf_score), np.mean(inner_clf_score), C_best_i, C_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "agreed-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "####################################\n",
    "## SVM - grid search - two splits ##\n",
    "####################################\n",
    "\"\"\"\n",
    "def svm_grid_2s(bold_data, label_data, norm):\n",
    "    X = np.concatenate(bold_data)\n",
    "    y = np.concatenate(label_data)\n",
    "    len_dat = int(bold_data[0].shape[0]) + int(bold_data[1].shape[0])\n",
    "    #X = bold_data\n",
    "    #y = label_data\n",
    "    groups = np.hstack(([0]*num_trs, [1]*num_trs,[2]*num_trs,[3]*num_trs))\n",
    "    kf = KFold(n_splits=2)\n",
    "\n",
    "    # leave out runs without two conditions (train on cond a, cond a, test on cond b, cond b)\n",
    "    # determine which runs to leave out by skipping runs that contain an index from the first\n",
    "    # run AND the third, OR an index from the second and fourth\n",
    "    # first index of each run in the entire array\n",
    "    run1_ind = int(len_dat * 0)\n",
    "    run2_ind = int(len_dat * .25)\n",
    "    run3_ind = int(len_dat * .5)\n",
    "    run4_ind = int(len_dat * .75)\n",
    "\n",
    "    clf_score = np.array([])\n",
    "    inner_clf_score = np.array([])\n",
    "    C_best = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X, y, groups):\n",
    "\n",
    "        # Train test sets # \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Normalize data   \n",
    "        if norm:\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        \n",
    "        \n",
    "\n",
    "        # For the present train vs test set, cross validate\n",
    "        parameters = {'C':[0.00001, 0.0001, 0.01, 0.1, 1, 10]}\n",
    "        inner_clf = GridSearchCV(\n",
    "            SVC(kernel='linear'),\n",
    "            parameters,\n",
    "            cv=5,\n",
    "            return_train_score=True)\n",
    "        inner_clf.fit(X_train, y_train)\n",
    "        #print(\"inner score: \", inner_clf.score(X_train, y_train))\n",
    "        inner_clf_score = np.hstack((inner_clf_score, inner_clf.score(X_train, y_train)))\n",
    "\n",
    "        # Find the best hyperparameter\n",
    "        C_best_i = inner_clf.best_params_['C']\n",
    "        C_best.append(C_best_i)\n",
    "\n",
    "        # Train the classifier with the best hyperparameter using training and validation set\n",
    "        classifier = SVC(kernel=\"linear\", C=C_best_i)\n",
    "        clf = classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Test the classifier\n",
    "        score = clf.score(X_test, y_test)\n",
    "        clf_score = np.hstack((clf_score, score))\n",
    "\n",
    "    #print ('Inner loop classification accuracy:', np.mean(inner_clf_score))\n",
    "    #print('best c: ', C_best_i)\n",
    "    #print ('Overall accuracy: ', np.mean(clf_score))\n",
    "    return np.mean(clf_score), np.mean(inner_clf_score), C_best_i, C_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "superior-messaging",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "####################################\n",
    "## Log regress, 4 groups, L2 ## \n",
    "####################################\n",
    "\"\"\"\n",
    "def log_grid_4s_l2(bold_data, label_data, norm):\n",
    "    X = np.concatenate(bold_data)\n",
    "    y = np.concatenate(label_data)\n",
    "    len_dat = int(bold_data[0].shape[0]) + int(bold_data[1].shape[0])\n",
    "    #X = bold_data\n",
    "    #y = label_data\n",
    "    groups = np.hstack(([0]*num_trs, [1]*num_trs,[2]*num_trs,[3]*num_trs))\n",
    "    lpgo = LeavePGroupsOut(n_groups=2)\n",
    "    lpgo.get_n_splits(X, y, groups)\n",
    "    lpgo.get_n_splits(groups=groups)  # 'groups' is always required\n",
    "\n",
    "    # leave out runs without two conditions (train on cond a, cond a, test on cond b, cond b)\n",
    "    # determine which runs to leave out by skipping runs that contain an index from the first\n",
    "    # run AND the third, OR an index from the second and fourth\n",
    "    # first index of each run in the entire array\n",
    "    run1_ind = int(len_dat * 0)\n",
    "    run2_ind = int(len_dat * .25)\n",
    "    run3_ind = int(len_dat * .5)\n",
    "    run4_ind = int(len_dat * .75)\n",
    "\n",
    "    clf_score = np.array([])\n",
    "    inner_clf_score = np.array([])\n",
    "    C_best = []\n",
    "\n",
    "    for train_index, test_index in lpgo.split(X, y, groups):\n",
    "        #print(groups)\n",
    "        if run1_ind in train_index and run3_ind in train_index:\n",
    "            continue\n",
    "        if run2_ind in train_index and run4_ind in train_index:\n",
    "            continue\n",
    "\n",
    "        # Train test sets # \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Normalize data \n",
    "        if norm:\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        \n",
    "        \n",
    "\n",
    "        # For the present train vs test set, cross validate\n",
    "        parameters = {'C':[0.00001, 0.0001, 0.01, 0.1, 1, 10]}\n",
    "        inner_clf = GridSearchCV(\n",
    "            LogisticRegression(penalty='l2'),\n",
    "            parameters,\n",
    "            # does this matter at all? like the more folds and the more params the shittier the estimate? \n",
    "            cv=5,\n",
    "            return_train_score=True)\n",
    "        inner_clf.fit(X_train, y_train)\n",
    "        inner_clf_score = np.hstack((inner_clf_score, inner_clf.score(X_train, y_train)))\n",
    "        #print(\"inner score: \", inner_clf.score(X_train, y_train))\n",
    "\n",
    "        # Find the best hyperparameter\n",
    "        C_best_i = inner_clf.best_params_['C']\n",
    "        C_best.append(C_best_i)\n",
    "\n",
    "        # Train the classifier with the best hyperparameter using training and validation set\n",
    "        classifier = LogisticRegression(penalty='l2', C=C_best_i)\n",
    "        clf = classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Test the classifier\n",
    "        score = clf.score(X_test, y_test)\n",
    "        clf_score = np.hstack((clf_score, score))\n",
    "\n",
    "    #print ('Inner loop classification accuracy:', np.mean(inner_clf_score))\n",
    "    # c_best: what was the best c for each loop? \n",
    "    #print('best c: ', C_best)\n",
    "    #print ('Overall accuracy: ', np.mean(clf_score))\n",
    "    return np.mean(clf_score), np.mean(inner_clf_score), C_best_i, C_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "congressional-spider",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "####################################\n",
    "## Log regress, 2 groups, L2 ## \n",
    "####################################\n",
    "\"\"\"\n",
    "def log_grid_2s_l2(bold_data, label_data, norm):\n",
    "    X = np.concatenate(bold_data)\n",
    "    y = np.concatenate(label_data)\n",
    "    len_dat = int(bold_data[0].shape[0]) + int(bold_data[1].shape[0])\n",
    "    #X = bold_data\n",
    "    #y = label_data\n",
    "    groups = np.hstack(([0]*num_trs, [1]*num_trs,[2]*num_trs,[3]*num_trs))\n",
    "    kf = KFold(n_splits=2)\n",
    "\n",
    "\n",
    "    clf_score = np.array([])\n",
    "    inner_clf_score = np.array([])\n",
    "    C_best = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X, y, groups):\n",
    "\n",
    "        # Train test sets # \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Normalize data \n",
    "        if norm:\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        \"\"\"\n",
    "        print(\"X_train: \", train_index, \"X_test\", test_index)\n",
    "        print()\n",
    "        print(\"Y_train: \", y_train, \"y_test\", y_test)\n",
    "        \"\"\"\n",
    "        # For the present train vs test set, cross validate\n",
    "        parameters = {'C':[0.00001, 0.0001, 0.01, 0.1, 1, 10]}\n",
    "        inner_clf = GridSearchCV(\n",
    "            LogisticRegression(penalty='l2'),\n",
    "            parameters,\n",
    "            # does this matter at all? like the more folds and the more params the shittier the estimate? \n",
    "            cv=5,\n",
    "            return_train_score=True)\n",
    "        inner_clf.fit(X_train, y_train)\n",
    "        inner_clf_score = np.hstack((inner_clf_score, inner_clf.score(X_train, y_train)))\n",
    "        #print(\"inner score: \", inner_clf.score(X_train, y_train))\n",
    "\n",
    "        # Find the best hyperparameter\n",
    "        C_best_i = inner_clf.best_params_['C']\n",
    "        C_best.append(C_best_i)\n",
    "\n",
    "        # Train the classifier with the best hyperparameter using training and validation set\n",
    "        classifier = LogisticRegression(penalty='l2', C=C_best_i)\n",
    "        clf = classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Test the classifier\n",
    "        score = clf.score(X_test, y_test)\n",
    "        clf_score = np.hstack((clf_score, score))\n",
    "\n",
    "    #print ('Inner loop classification accuracy:', np.mean(inner_clf_score))\n",
    "    # c_best: what was the best c for each loop? \n",
    "    #print('best c: ', C_best)\n",
    "    #print ('Overall accuracy: ', np.mean(clf_score))\n",
    "    return np.mean(clf_score), np.mean(inner_clf_score), C_best_i, C_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fifth-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "####################################\n",
    "## Log regress, 4 groups, L1  ## \n",
    "####################################\n",
    "\"\"\"\n",
    "def log_grid_4s_l1(bold_data, label_data, norm):\n",
    "    X = np.concatenate(bold_data)\n",
    "    y = np.concatenate(label_data)\n",
    "    len_dat = int(bold_data[0].shape[0]) + int(bold_data[1].shape[0])\n",
    "    #X = bold_data\n",
    "    #y = label_data\n",
    "    groups = np.hstack(([0]*num_trs, [1]*num_trs,[2]*num_trs,[3]*num_trs))\n",
    "    lpgo = LeavePGroupsOut(n_groups=2)\n",
    "    lpgo.get_n_splits(X, y, groups)\n",
    "    lpgo.get_n_splits(groups=groups)  # 'groups' is always required\n",
    "\n",
    "\n",
    "    # leave out runs without two conditions (train on cond a, cond a, test on cond b, cond b)\n",
    "    # determine which runs to leave out by skipping runs that contain an index from the first\n",
    "    # run AND the third, OR an index from the second and fourth\n",
    "    # first index of each run in the entire array\n",
    "    run1_ind = int(len_dat * 0)\n",
    "    run2_ind = int(len_dat * .25)\n",
    "    run3_ind = int(len_dat * .5)\n",
    "    run4_ind = int(len_dat * .75)\n",
    "\n",
    "    clf_score = np.array([])\n",
    "    inner_clf_score = np.array([])\n",
    "    C_best = []\n",
    "\n",
    "    for train_index, test_index in lpgo.split(X, y, groups):\n",
    "        #print(groups)\n",
    "        if run1_ind in train_index and run3_ind in train_index:\n",
    "            continue\n",
    "        if run2_ind in train_index and run4_ind in train_index:\n",
    "            continue\n",
    "\n",
    "        # Train test sets # \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Normalize data \n",
    "        if norm:\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        \n",
    "\n",
    "        # For the present train vs test set, cross validate\n",
    "        parameters = {'C':[0.00001, 0.0001, 0.01, 0.1, 1, 10]}\n",
    "        inner_clf = GridSearchCV(\n",
    "            LogisticRegression(penalty=\"l1\", solver=\"saga\", max_iter=5000),\n",
    "            parameters,\n",
    "            # does this matter at all? like the more folds and the more params the shittier the estimate? \n",
    "            cv=5,\n",
    "            return_train_score=True)\n",
    "        inner_clf.fit(X_train, y_train)\n",
    "        #print(\"inner score: \", inner_clf.score(X_train, y_train))\n",
    "        inner_clf_score = np.hstack((inner_clf_score, inner_clf.score(X_train, y_train)))\n",
    "\n",
    "        # Find the best hyperparameter\n",
    "        C_best_i = inner_clf.best_params_['C']\n",
    "        C_best.append(C_best_i)\n",
    "\n",
    "        # Train the classifier with the best hyperparameter using training and validation set\n",
    "        classifier = LogisticRegression(penalty=\"l1\", C=C_best_i, solver=\"saga\", max_iter=5000)\n",
    "        clf = classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Test the classifier\n",
    "        score = clf.score(X_test, y_test)\n",
    "        clf_score = np.hstack((clf_score, score))\n",
    "\n",
    "    #print ('Inner loop classification accuracy:', np.mean(inner_clf_score))\n",
    "    # c_best: what was the best c for each loop? \n",
    "    #print('best c: ', C_best)\n",
    "    #print ('Overall accuracy: ', np.mean(clf_score))\n",
    "    return np.mean(clf_score), np.mean(inner_clf_score), C_best_i, C_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "motivated-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_fMRI_dat(concatenated_data, num_trs):\n",
    "    cond_data= []\n",
    "    vox = concatenated_data[0].shape[1]\n",
    "    total_tr = concatenated_data[0].shape[0]\n",
    "    # Average across x TRs(e.g.4 -- 6 seconds)  \n",
    "    div_tr = int(200 / num_trs)\n",
    "    if num_trs != 200:\n",
    "        # Each row represents a timepoint (TR), each column a voxel\n",
    "        # get the number of voxels, tr\n",
    "        #### Condense Data ### \n",
    "        ## For every 8 timeponts (TRs), average the TRs to one activation value for that particular voxel ##\n",
    "        ## 200 timepoints / 8 becomes 25 timepoints to be fed into the classifer ##\n",
    "\n",
    "        # Create empty dataframe to be filled with each runs condensed values # \n",
    "        \n",
    "        for run in range(0,len(concatenated_data)):\n",
    "            # create dataframe of values per run # \n",
    "            df = pd.DataFrame(concatenated_data[run])\n",
    "            # create empty dataframe to be filled with each runs condensed data\n",
    "            tem2 = pd.DataFrame({'dummy': [0]*int((total_tr / div_tr))})\n",
    "            # For each voxel (col), create 25 datapoints (TRs)\n",
    "            for col in range(0,vox):\n",
    "                # empty array to store mean of every 8 points \n",
    "                tem1 = []\n",
    "                # Iterate through columns, NOT rows. Thats why we use pd\n",
    "                c = df[col]\n",
    "                # For every div_tr (8), take the average\n",
    "                for i in range(0, len(c)-1, div_tr):\n",
    "                    tem1.append(statistics.mean(c[i:i+div_tr]))\n",
    "                # Insert averaged values into temporary dframe\n",
    "                tem2.insert(len(tem2.columns), col, tem1)\n",
    "            # Delete dummy column and flatten back to np array\n",
    "            run_data = tem2.iloc[:,1:].values\n",
    "            # append run data into a new list of all runs condensed data\n",
    "            cond_data.append(run_data)\n",
    "        print(\"Run \", run, \"- TRs: \", cond_data[0].shape[0], \"voxels: \",cond_data[0].shape[1])\n",
    "    else:\n",
    "        cond_data = concatenated_data\n",
    "        print(\"TRs: \", num_trs, \"Vox: \", vox)\n",
    "    return cond_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "peaceful-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epi_data(sub, ses, task,run, space):\n",
    "  # Load MRI file\n",
    "    if space == \"MNI\":\n",
    "        epi_in = os.path.join(data_dir, sub, ses, 'func', \"%s_%s_task-%s_run-%s_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\" % (sub, ses, task,run))\n",
    "    elif space == \"T1\":\n",
    "        epi_in = os.path.join(data_dir, sub, ses, 'func', \"%s_%s_task-%s_run-%s_space-T1w_desc-preproc_bold.nii.gz\" % (sub, ses, task,run))\n",
    "    else:\n",
    "        print(\"wrong load epi input. check this function\")\n",
    "    epi_data = nib.load(epi_in)\n",
    "    print(\"Loading data from %s\" % (epi_in))\n",
    "    return epi_data\n",
    "\n",
    "def load_roi_mask(ROI_name, space):\n",
    "    if space == \"MNI\":\n",
    "        maskdir = os.path.join(rois_dir)    \n",
    "        print(\"expected shape: 78, 93,65\")\n",
    "    elif space == \"T1\":\n",
    "        maskdir = os.path.join(rois_dir+ \"/T1\")\n",
    "        print(\"expected shape: 56, 72,53\")\n",
    "    else:\n",
    "        print(\"wrong mask input. check this function\")\n",
    "    # load the mask\n",
    "    maskfile = os.path.join(maskdir, \"%s.nii\" % (ROI_name))\n",
    "    mask = nib.load(maskfile)\n",
    "    print(\"mask shape: \", mask.shape)\n",
    "    print(\"Loaded %s mask\" % (ROI_name))\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "unauthorized-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fMRI(sub, ROI_name, num_runs,reg, norm_type, conf=False, ses=\"ses-01\",task=\"Attn\"):\n",
    "    # This is based off of 'load_data' function in template\n",
    "    # Loads all fMRI runs into a matrix #\n",
    "    \"\"\"\n",
    "    reg = T1 or MNI registration?\n",
    "    norm_type = by Space or by Time? \n",
    "    \"\"\"\n",
    "    concatenated_data = []\n",
    "    print(ROI_name)\n",
    "    for run in range(1, num_runs + 1):\n",
    "        # Load epi data \n",
    "        epi = load_epi_data(sub,ses,task,run,reg)\n",
    "        # Load ROI data\n",
    "        roi_samp =load_roi_mask(ROI_name,reg)\n",
    "        # Pull voxels from epi data # *** may need to change this \n",
    "        nifti_masker = NiftiMasker(mask_img=roi_samp,high_pass=1/128, t_r=1.5)\n",
    "        maskedData = nifti_masker.fit_transform(epi)\n",
    "        # Normalize data # \n",
    "        if norm_type == \"space\":\n",
    "            scaler = preprocessing.StandardScaler().fit(maskedData)\n",
    "            preprocessed_data = scaler.transform(maskedData)\n",
    "        else:\n",
    "            scaler = preprocessing.StandardScaler().fit(maskedData.T)\n",
    "            preprocessed_data = scaler.transform(maskedData.T)\n",
    "            preprocessed_data = preprocessed_data.T\n",
    "        # Regress Confounds #\n",
    "        if conf:\n",
    "            run_conf = np.asarray(pd.read_csv(os.path.join(confounds + sub + \"/func/\", '%s_ses-01_task-Attn_run-%s_desc-model_timeseries.csv') % (sub, run)))\n",
    "            preprocessed_data = nil.signal.clean(preprocessed_data, detrend=False, t_r = 1.5, standardize=False, confounds=run_conf, standardize_confounds=True)\n",
    "        \n",
    "        # clean the first 9 TRs\n",
    "        concatenated_data.append(preprocessed_data[9:])\n",
    "        #print('TRs: ', concatenated_data.shape[1], '; Voxels: ', concatenated_data.shape[0])\n",
    "\n",
    "        print(\"run \", run, \" finished...\")\n",
    "    print(\"Number of Runs: \",len(concatenated_data), \" Number of voxels: \", concatenated_data[0].shape[1], \" Number of TRs:\",\n",
    "     concatenated_data[0].shape[0])\n",
    "    return concatenated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "resident-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fMRI2(sub, ROI_name, num_runs,reg, norm_type, conf=False, ses=\"ses-01\",task=\"Attn\"):\n",
    "    # This is based off of 'load_data' function in template\n",
    "    # Loads all fMRI runs into a matrix #\n",
    "    \"\"\"\n",
    "    reg = T1 or MNI registration?\n",
    "    norm_type = by Space or by Time? \n",
    "    \"\"\"\n",
    "    concatenated_data = []\n",
    "    print(ROI_name)\n",
    "    for run in range(1, num_runs + 1):\n",
    "        # Load epi data \n",
    "        epi = load_epi_data(sub,ses,task,run,reg)\n",
    "        # Load ROI data\n",
    "        roi_samp =load_roi_mask(ROI_name,reg)\n",
    "        # Pull voxels from epi data # *** may need to change this \n",
    "        nifti_masker = NiftiMasker(mask_img=roi_samp, t_r=1.5)\n",
    "        maskedData = nifti_masker.fit_transform(epi)\n",
    "        \n",
    "        # Regress Confounds #\n",
    "        if conf:\n",
    "            print(\"regressing confounds!\")\n",
    "            run_conf = np.asarray(pd.read_csv(os.path.join(confounds + sub + \"/func/\", \n",
    "                                                           '%s_ses-01_task-Attn_run-%s_desc-model_timeseries.csv') % (sub, run)))\n",
    "            preprocessed_data = nil.signal.clean(maskedData, detrend=False, t_r = 1.5, standardize=True, \n",
    "                                                 confounds=run_conf, standardize_confounds=True, high_pass=1/128)\n",
    "        else:\n",
    "            preprocessed_data = nil.signal.clean(maskedData, detrend=False, t_r = 1.5, standardize=True, \n",
    "                                                 high_pass=1/128)\n",
    "        # Normalize data by time # \n",
    "        # *** Turn off standardize above if I want to standardize by time\n",
    "        if norm_type != \"space\":\n",
    "            scaler = preprocessing.StandardScaler().fit(preprocessed_data.T)\n",
    "            preprocessed_data = scaler.transform(preprocessed_data.T)\n",
    "            preprocessed_data = preprocessed_data.T\n",
    "        \n",
    "        # clean the first 9 TRs\n",
    "        concatenated_data.append(preprocessed_data[9:])\n",
    "        #print('TRs: ', concatenated_data.shape[1], '; Voxels: ', concatenated_data.shape[0])\n",
    "\n",
    "        print(\"run \", run, \" finished...\")\n",
    "    print(\"Number of Runs: \",len(concatenated_data), \" Number of voxels: \", concatenated_data[0].shape[1], \" Number of TRs:\",\n",
    "     concatenated_data[0].shape[0])\n",
    "    return concatenated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-offense",
   "metadata": {},
   "source": [
    "# Define Static VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "respective-brown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTo do:\\n- log regression\\n- optimize hyper parameters\\n- what accuracies hsould i be expecting\\n- only train and test on two\\n- bootstrap\\n- two splits, not four \\n- Try averagin datapoints \\n- motion\\n- test on multiple subjects \\n- subject specific space \\n- implement z score during training\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "To do:\n",
    "- log regression\n",
    "- optimize hyper parameters\n",
    "- what accuracies hsould i be expecting\n",
    "- only train and test on two\n",
    "- bootstrap\n",
    "- two splits, not four \n",
    "- Try averagin datapoints \n",
    "- motion\n",
    "- test on multiple subjects \n",
    "- subject specific space \n",
    "- implement z score during training\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bearing-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "data_dir = \"/jukebox/graziano/coolCatIsaac/ATM/data/bids/derivatives/fmriprep/\"\n",
    "rois_dir = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/rois/\"\n",
    "behav_p = '/jukebox/graziano/coolCatIsaac/ATM/data/behavioral'\n",
    "sav_work = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/results/\"\n",
    "confounds = '/jukebox/graziano/coolCatIsaac/ATM/data/bids/derivatives/fmriprep/afni-head_mot/'\n",
    "output_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-execution",
   "metadata": {},
   "source": [
    "# Dynamic vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "simplified-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GENERAL ####\n",
    "sub_list = [\"sub-000\"]\n",
    "roi_list =[\"meta_L_STS_10mm_resampled\",\"meta_MPFC_10mm_resampled\", \"meta_R_STS_10mm_resampled\",\n",
    "           \"meta_L_TPJ_10mm_resampled\", \"meta_Prec_10mm_resampled\",  \"meta_R_TPJ_10mm_resampled\"]\n",
    "\n",
    "###### LOADING VARS #######\n",
    "# Number of runs to load \n",
    "num_runs = 10\n",
    "# Registration ust be either T1 or MNI\n",
    "reg = \"MNI\"\n",
    "# Registration Space # \n",
    "norm_type = \"space\"\n",
    "# load regressed file ? \n",
    "conf_y_n = \"conf-yes\" #\"conf-no\"\n",
    "\n",
    "\n",
    "###### ANALYSIS VARS #######\n",
    "# number of bootstraps #\n",
    "num_strap = 1000\n",
    "# Conditions\n",
    "cond_a = 'OM'\n",
    "cond_b = 'OC'\n",
    "# Number of TRS to include in a run (e.g. condense from 200 --> x by averaging ## \n",
    "num_tr_list = [100]\n",
    "# Normalize train and test sets? \n",
    "two_norm = True\n",
    "# Number of times normalized (for labeling)\n",
    "norm = \"z=1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-birth",
   "metadata": {},
   "source": [
    "# Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "conceptual-vitamin",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run  9 - TRs:  100 voxels:  208\n",
      "number of runs:  2 bold shape:  (200, 208) label shape:  (200,)\n",
      "Expected shape:  200\n",
      "Extracted:  OM OC\n",
      "sub finished!\n",
      "Run  9 - TRs:  100 voxels:  212\n",
      "number of runs:  2 bold shape:  (200, 212) label shape:  (200,)\n",
      "Expected shape:  200\n",
      "Extracted:  OM OC\n",
      "sub finished!\n",
      "Run  9 - TRs:  100 voxels:  219\n",
      "number of runs:  2 bold shape:  (200, 219) label shape:  (200,)\n",
      "Expected shape:  200\n",
      "Extracted:  OM OC\n",
      "sub finished!\n",
      "Run  9 - TRs:  100 voxels:  212\n",
      "number of runs:  2 bold shape:  (200, 212) label shape:  (200,)\n",
      "Expected shape:  200\n",
      "Extracted:  OM OC\n",
      "sub finished!\n",
      "Run  9 - TRs:  100 voxels:  208\n",
      "number of runs:  2 bold shape:  (200, 208) label shape:  (200,)\n",
      "Expected shape:  200\n",
      "Extracted:  OM OC\n",
      "sub finished!\n",
      "Run  9 - TRs:  100 voxels:  212\n",
      "number of runs:  2 bold shape:  (200, 212) label shape:  (200,)\n",
      "Expected shape:  200\n",
      "Extracted:  OM OC\n",
      "sub finished!\n"
     ]
    }
   ],
   "source": [
    "##### TO CHECK ######\n",
    "\"\"\"\n",
    " * ROI is correct\n",
    " * correct nomralization in all my classifier functions\n",
    " * bold data input np array is correct below\n",
    " * misc naming variables are correct above\n",
    "\"\"\"\n",
    "for roi in roi_list:\n",
    "    for sub in sub_list:\n",
    "        # Load Behavioral\n",
    "        behav = pd.read_csv(os.path.join(behav_p, '%s_behav_cleaned.csv') % (sub))\n",
    "        # Load fMRI\n",
    "        concatenated_data = list(np.load(sav_work + sub + \"_\"+ roi + \"_\" + reg + \"_\" + norm_type + \"_\"+ conf_y_n + \".npy\"))\n",
    "        # Define the column in behav to be used for creating labels # \n",
    "        label = behav.iloc[:,1]\n",
    "        for num_trs in num_tr_list:\n",
    "            # Create an array of labels [0] AND the order in which runs occured [1]#\n",
    "            sub_ses_labels = label_lists(label, num_trs)\n",
    "            # Average fMRI data according to num_trs\n",
    "            cond_data = avg_fMRI_dat(concatenated_data, num_trs)\n",
    "            ## Find run sequence, extraction condition indexes from behav data ## \n",
    "            lab_indic = find_cond_index(sub_ses_labels[0])\n",
    "            # Organize labels and bold data by indices, prepare for classification # \n",
    "            bold_data, label_data = org_bold_data(cond_data, sub_ses_labels[1], lab_indic, cond_a, cond_b)\n",
    "            # Classify w four models\n",
    "            for boot in range(0,num_strap):\n",
    "                #print(\"Before: \", label_data)\n",
    "                ## Shuffle TRs + classify\n",
    "                for i in range(0,2):\n",
    "                    np.random.shuffle(label_data[i])\n",
    "                #print(\"After: \", label_data)\n",
    "                output_acc = [svm_grid_4s(bold_data, label_data,two_norm),\n",
    "                              svm_grid_2s(bold_data, label_data,two_norm),\n",
    "                              log_grid_2s_l2(bold_data, label_data,two_norm),\n",
    "                              log_grid_4s_l2(bold_data, label_data,two_norm)]\n",
    "                output_data.append(np.asarray((sub,output_acc[0][0], \n",
    "                                           output_acc[1][0],\n",
    "                                           output_acc[2][0], \n",
    "                                           output_acc[3][0], \n",
    "                                           np.hstack((cond_a, cond_b)),\n",
    "                                           num_trs, roi, reg, two_norm)))\n",
    "\n",
    "        print(\"sub finished!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-hammer",
   "metadata": {},
   "source": [
    "# view results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "incorporated-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_lab = np.hstack(('svm_grid_4s','svm_grid_2s','log_grid_2s_l2', 'log_grid_4s_l2'))\n",
    "df_lab = np.hstack((\"sub\",np.asarray(output_lab), \"cond\", \"num_trs\", \"roi_name\", \"reg\", \"two norm?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "available-pattern",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(np.asarray(output_data), columns=df_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "limited-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "xport = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "respected-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "xport.to_csv(sav_work + \"/boot/\"  + \"boot_jul30_RESULTS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "comparable-daniel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook analysis_BOOT_7-30.ipynb to script\n",
      "[NbConvertApp] Writing 31036 bytes to analysis_BOOT_7-30.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script analysis_BOOT_7-30.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-interval",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

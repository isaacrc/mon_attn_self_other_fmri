{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "disturbed-housing",
   "metadata": {},
   "source": [
    "# GLM Between Analyis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-astrology",
   "metadata": {},
   "source": [
    "This script performs within analysis including:\n",
    "- SM win 3,2,1 0 minus SC win 3,2,1, 0\n",
    "- OM win 3,2,1 0 minus OC win 3,2,1, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-freedom",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hidden-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Standard Library\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import statistics\n",
    "from copy import deepcopy\n",
    "\n",
    "# NumPy, SciPy, Pandas\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, zscore, pearsonr, sem\n",
    "from scipy.signal import gaussian, convolve\n",
    "from scipy.spatial.distance import squareform\n",
    "import pandas as pd\n",
    "\n",
    "# Matplotlib & Seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "sns.set(style='white', context='talk', font_scale=1, rc={\"lines.linewidth\": 2})\n",
    "\n",
    "# Nibabel\n",
    "import nibabel as nib\n",
    "\n",
    "# Nilearn\n",
    "from nilearn import image, masking, datasets, plotting, connectome, input_data\n",
    "from nilearn.input_data import (\n",
    "    NiftiMasker, MultiNiftiMasker, NiftiLabelsMasker, NiftiSpheresMasker\n",
    ")\n",
    "from nilearn.masking import compute_epi_mask, apply_mask\n",
    "from nilearn.image import (\n",
    "    concat_imgs, resample_img, mean_img, index_img, resample_to_img, math_img\n",
    ")\n",
    "from nilearn.plotting import plot_roi, plot_glass_brain, view_img\n",
    "from nilearn.glm.first_level import FirstLevelModel, make_first_level_design_matrix\n",
    "\n",
    "# Brainiak\n",
    "from brainiak import image as brainiak_image, io as brainiak_io\n",
    "import brainiak.eventseg.event\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn import preprocessing, decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import (\n",
    "    PredefinedSplit, LeaveOneOut, KFold, GroupShuffleSplit,\n",
    "    LeavePGroupsOut, LeaveOneGroupOut, train_test_split\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, chi2, f_classif, VarianceThreshold\n",
    ")\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# DeepDish\n",
    "import deepdish as dd\n",
    "\n",
    "# Statsmodels\n",
    "import statsmodels.stats.multitest as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "present-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import label_lists, find_cond_index, load_epi_data, load_roi_mask,intersect_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-transaction",
   "metadata": {},
   "source": [
    "# Custom Study Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "guilty-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(sub, behav_path):\n",
    "    \"\"\"\n",
    "    Load subject behavioral CSV and extract condition labels.\n",
    "    \n",
    "    Args:\n",
    "        sub (str): Subject identifier.\n",
    "        behav_path (str): Path to the behavioral data directory.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of condition names mapping to run indices.\n",
    "    \"\"\"\n",
    "    behav = pd.read_csv(os.path.join(behav_path, f'{sub}_behav_cleaned.csv'))\n",
    "    labels = behav.iloc[:, 1]\n",
    "    _, run_order = label_lists(labels, 200)  # Assuming label_lists returns (labels, order)\n",
    "    return find_condition_indices(run_order)\n",
    "\n",
    "def organize_bold_data(bold_unsorted, run_indices, cond_a, cond_b):\n",
    "    \"\"\"\n",
    "    Reorder BOLD data based on condition-specific run indices.\n",
    "    \n",
    "    Args:\n",
    "        bold_unsorted (list): List of unsorted BOLD data arrays.\n",
    "        run_indices (dict): Mapping of condition labels to run indices.\n",
    "        cond_a (str): First condition label.\n",
    "        cond_b (str): Second condition label.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Arrays for cond_a and cond_b, each with two runs.\n",
    "    \"\"\"\n",
    "    a_runs = [bold_unsorted[run_indices[cond_a][0]], bold_unsorted[run_indices[cond_a][1]]]\n",
    "    b_runs = [bold_unsorted[run_indices[cond_b][0]], bold_unsorted[run_indices[cond_b][1]]]\n",
    "    print(f\"Returning runs for conditions: {cond_a}, {cond_b}\")\n",
    "    return np.asarray(a_runs), np.asarray(b_runs)\n",
    "\n",
    "def find_condition_indices(run_labels):\n",
    "    \"\"\"\n",
    "    Find run indices for each condition label.\n",
    "    \n",
    "    Args:\n",
    "        run_labels (list): Ordered run condition labels.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping from condition to list of run indices.\n",
    "    \"\"\"\n",
    "    conditions = {'SM': [], 'SC': [], 'OM': [], 'OC': [], 'RE': [0, 9]}\n",
    "    \n",
    "    for idx, label in enumerate(run_labels):\n",
    "        if label in conditions:\n",
    "            conditions[label].append(idx)\n",
    "    \n",
    "    return conditions\n",
    "\n",
    "def load_confounds(cond_list, sub_list, behav_path, confound_path):\n",
    "    \"\"\"\n",
    "    Load confound time series for each subject and condition.\n",
    "\n",
    "    Returns:\n",
    "        dict: conf_sub[sub][cond][run]\n",
    "    \"\"\"\n",
    "    conf_data = {}\n",
    "\n",
    "    for sub in sub_list:\n",
    "        cond_data = {}\n",
    "        run_indices = find_indices(sub, behav_path)\n",
    "\n",
    "        for cond in cond_list:\n",
    "            runs = []\n",
    "            for run_idx in run_indices[cond]:\n",
    "                fpath = os.path.join(\n",
    "                    confound_path, sub, \"func\",\n",
    "                    f\"{sub}_ses-01_task-Attn_run-{run_idx}_desc-model_timeseries.csv\"\n",
    "                )\n",
    "                data = pd.read_csv(fpath).iloc[4:].values\n",
    "                runs.append(data)\n",
    "            cond_data[cond] = runs\n",
    "        \n",
    "        conf_data[sub] = cond_data\n",
    "\n",
    "    return conf_data\n",
    "\n",
    "def load_two_condition_runs(sub, behav_path, cond_a, cond_b, run_dict, suffix=\"_205_noproc.npy\"):\n",
    "    \"\"\"\n",
    "    Load and organize BOLD data for two conditions (two runs each).\n",
    "    \n",
    "    Returns:\n",
    "        list: Two lists of 2D arrays, one for each condition.\n",
    "    \"\"\"\n",
    "    cats = list(np.load(load_frmi + sub + suffix, allow_pickle=True))\n",
    "    run_indices = find_indices(sub, behav_path)\n",
    "    a_runs, b_runs = organize_bold_data(cats, run_indices, cond_a, cond_b)\n",
    "    return list(a_runs[run_dict[sub][cond_a]]), list(b_runs[run_dict[sub][cond_b]])\n",
    "\n",
    "def create_event_list(sub, bpress, cond, run_dict, base_onset, comp_onsets, stim_dur):\n",
    "    \"\"\"\n",
    "    Generate event onsets for each run of a subject.\n",
    "    \n",
    "    Returns:\n",
    "        dict: run-indexed event DataFrames.\n",
    "        list: window labels used.\n",
    "    \"\"\"\n",
    "    all_onsets = list(np.asarray(bpress[sub][cond])[run_dict[sub][cond]])\n",
    "    events_dict = {}\n",
    "    window_labels = []\n",
    "\n",
    "    for run_idx, onset_array in enumerate(all_onsets):\n",
    "        onset_array = onset_array.astype(float)\n",
    "        trial_types = ['win0'] * len(onset_array)\n",
    "        durations = [stim_dur] * len(onset_array)\n",
    "        onsets = [t + base_onset for t in onset_array]\n",
    "\n",
    "        for i, comp_onset in enumerate(comp_onsets):\n",
    "            label = f'win{i+1}'\n",
    "            trial_types += [label] * len(onset_array)\n",
    "            durations += [stim_dur] * len(onset_array)\n",
    "            onsets += [t + comp_onset for t in onset_array]\n",
    "            window_labels.append(label)\n",
    "\n",
    "        events_df = pd.DataFrame({\n",
    "            'onset': onsets,\n",
    "            'trial_type': trial_types,\n",
    "            'duration': durations\n",
    "        })\n",
    "        events_dict[run_idx] = events_df\n",
    "\n",
    "    return events_dict, window_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-living",
   "metadata": {},
   "source": [
    "# Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surgical-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data/bids' # where the fMRI data is downloaded\n",
    "rois_dir = ... # not specified \n",
    "behav_p = '../behavioral/stim_list'\n",
    "load_bpress = \"../behavioral/\"\n",
    "load_fmri = '...' #location of preprocessed data output from preproc scripts\n",
    "out_dir = \"../../data/output\" # where to save the directory\n",
    "confounds_dir = '../behavioral_data/confound_info/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-sentence",
   "metadata": {},
   "source": [
    "# Sublist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "married-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list = [\"sub-000\",\"sub-001\",\"sub-002\",\"sub-003\",\"sub-004\",\"sub-005\",\"sub-006\",\"sub-007\",\"sub-008\",\"sub-009\",\n",
    "            \"sub-010\",\"sub-011\",\"sub-012\",\"sub-013\",\"sub-014\",\"sub-015\", \"sub-016\",\"sub-017\", \n",
    "            \"sub-018\", \"sub-019\", \"sub-020\",\"sub-021\",'sub-022','sub-023','sub-024','sub-025','sub-026','sub-027']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-swedish",
   "metadata": {},
   "source": [
    "# Analysis variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "raising-browse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: '/jukebox/graziano/coolCatIsaac/ATM/data/work/results/bpress_GLM/n28_p3_zscr_4fhwm_hp001p25_shaefGM_excOvlp'\n"
     ]
    }
   ],
   "source": [
    "# ** Only need to change inputs below!\n",
    "hm_thresh = str(3) ## framewise displacement threshold \n",
    "prefix = 'between'\n",
    "sav_dir = 'n28_p'+hm_thresh+'_betas_4fhwm_hp001p25_shaefGM_excOvlp'\n",
    "\n",
    "# start of zero-th window in seconds\n",
    "base_onset = -6\n",
    "# how many windows and what is the onset start time\n",
    "# we're now centering on the TR in which the bpress occured - onset of win 1\n",
    "# -3-3,-3+3,-3+6\n",
    "comp_onset_list = [-3,0,3]\n",
    "stim_dur = 3 # how long is the stimuous on\n",
    "\n",
    "# set the conditions conditions #\n",
    "cond_all = np.asarray([['SM','SC'],['OM', 'OC']])\n",
    "# In seconds what is the lag between a stimulus onset and the peak bold response\n",
    "sec_lag = 0\n",
    "\n",
    "# Create dir\n",
    "path = out_dir+sav_dir\n",
    "try: \n",
    "    os.mkdir(os.path.join(path))\n",
    "except OSError as error: \n",
    "    print(error)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-civilization",
   "metadata": {},
   "source": [
    "# Static variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "personal-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = 1.5  # repetition time is 1 second\n",
    "n_scans = 205 # the acquisition comprises 204 scans\n",
    "frame_times = np.arange(n_scans) * tr  # here are the correspoding frame times given TRs\n",
    "# subject dics #\n",
    "group_sub_glm_a = {}\n",
    "group_sub_glm_b = {}\n",
    "# sublist for individ analysis below # \n",
    "excl_sub_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "focal-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD MASK # \n",
    "# mask in study was derived f rom schaeffer parcellation. To find analagous grey matter mask, see: https://brainiak.org/tutorials/01-setup/\n",
    "# alternatively, see the script 'generate_masks'\n",
    "gm_shaef = nib.load('../../data/shaef_gm_MNI_mask.nii')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "increased-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET BUTTON PRESS INFO PER SUBJECT  ##\n",
    "bpress = dict(enumerate(np.load(os.path.join(load_bpress, \"n28_4_conds_ts_press_ovrlpREMOV.npy\"), \n",
    "                                allow_pickle=True).flatten(),1))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "studied-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "### runs to exclude with head motion accounted for and missing bpress runs deleted\n",
    "run_dic = dict(enumerate(np.load(os.path.join(confounds_dir, \"n28_runs_2_include_removNoBpress_delHMruns_threshp%s.npy\") %(hm_thresh), \n",
    "                                allow_pickle=True).flatten(),1))[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load confounds \n",
    "conf_sub = dict(enumerate(np.load(os.path.join(confounds_dir, 'n28_conf+cens_MERGE_removNoBpress_delHMruns_threshp%s_glm.npy')%(hm_thresh), \n",
    "                                          allow_pickle = True).flatten(),1))[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-kingston",
   "metadata": {},
   "source": [
    "# GLM variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sexual-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_glm = FirstLevelModel(t_r=1.5,\n",
    "                           signal_scaling=False,\n",
    "                           hrf_model = 'glover',\n",
    "                           drift_order=None,\n",
    "                           mask_img = gm_shaef,\n",
    "                           high_pass=None,\n",
    "                           drift_model=None,\n",
    "                           smoothing_fwhm=4,\n",
    "                           standardize=True,\n",
    "                           minimize_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "perceived-friday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'drift_model': None,\n",
       " 'drift_order': None,\n",
       " 'fir_delays': [0],\n",
       " 'high_pass': None,\n",
       " 'hrf_model': 'glover',\n",
       " 'mask_img': <nibabel.nifti1.Nifti1Image at 0x7f0970c733d0>,\n",
       " 'memory': Memory(location=None),\n",
       " 'memory_level': 1,\n",
       " 'min_onset': -24,\n",
       " 'minimize_memory': False,\n",
       " 'n_jobs': 1,\n",
       " 'noise_model': 'ar1',\n",
       " 'signal_scaling': False,\n",
       " 'slice_time_ref': 0.0,\n",
       " 'smoothing_fwhm': 4,\n",
       " 'standardize': True,\n",
       " 'subject_label': None,\n",
       " 't_r': 1.5,\n",
       " 'target_affine': None,\n",
       " 'target_shape': None,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmri_glm.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-dover",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-committee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur conds: ['OM' 'OC'] at HM thresh 3\n",
      "design mat for sub-015\n",
      "returning OM OC\n",
      "fit glm\n",
      "finish win0\n",
      "finish win1\n",
      "finish win2\n",
      "finish win3\n",
      "finish sub-015\n"
     ]
    }
   ],
   "source": [
    "for cond_list in cond_all:\n",
    "    cond_a = cond_list[0]\n",
    "    cond_b = cond_list[1]\n",
    "    print('cur conds:', cond_list, 'at HM thresh', hm_thresh)\n",
    "    \n",
    "    for sub in sub_list:\n",
    "        # skip sub if both runs for a condition are unavailable (see head_mot)\n",
    "        if conf_sub[sub][cond_a] == [] or conf_sub[sub][cond_b] == []: continue\n",
    "        excl_sub_list.append(sub)\n",
    "        print('design mat for', sub)\n",
    "        \n",
    "        sub_conts_a = {}\n",
    "        sub_conts_b = {}\n",
    "        # Create events for all four runs\n",
    "        events_a, window_list = create_event_list(sub, bpress, cond_a,run_dic, base_onset,comp_onset_list,stim_dur)\n",
    "       \n",
    "        # If rest condition, then use button presses from condition A \n",
    "        if cond_b == 'RE': \n",
    "            events_b,window_list = create_event_list(sub, bpress, cond_a,run_dic,base_onset,comp_onset_list,stim_dur)\n",
    "        else:\n",
    "            events_b,window_list = create_event_list(sub, bpress, cond_b,run_dic,base_onset,comp_onset_list,stim_dur)\n",
    "            \n",
    "        # Create design matrix - SM\n",
    "        design_matrices_a = [make_first_level_design_matrix(frame_times, events_a[df], hrf_model = 'glover',\n",
    "                      drift_model=None, add_regs=conf_sub[sub][cond_a][df]) for df in events_a]\n",
    "        # Create design matrix - SC (df +1 cuz regs are index 1,2, not 0, 1)\n",
    "        design_matrices_b = [make_first_level_design_matrix(frame_times, events_b[df], hrf_model = 'glover',\n",
    "                      drift_model=None, add_regs=conf_sub[sub][cond_b][df]) for df in events_b]\n",
    "\n",
    "        # FMRI - high pass, low pass filter\n",
    "        cond_a_runs_temp, cond_b_runs_temp = load_2conds_runs_fmri(sub,behav_p,cond_a,cond_b,run_dic)\n",
    "        cond_a_runs = [nil.image.clean_img(unclean_img, detrend=False, \n",
    "                                        standardize=False, \n",
    "                                        low_pass=.25,high_pass=.001,t_r=1.5) for unclean_img in cond_a_runs_temp]\n",
    "        cond_b_runs = [nil.image.clean_img(unclean_img, detrend=False, \n",
    "                                        standardize=False, \n",
    "                                        low_pass=.25,high_pass=.001,t_r=1.5) for unclean_img in cond_b_runs_temp]\n",
    "        \n",
    "\n",
    "        # FIT GLM per subject \n",
    "        print('fit glm')\n",
    "        window_list = ['win0','win1','win2','win3']\n",
    "        for win in window_list:\n",
    "            # group sub glm is now a nested dictionary with three contrasts\n",
    "            ## COND A ## \n",
    "            fmri_glm = fmri_glm.fit(cond_a_runs, design_matrices=design_matrices_a) \n",
    "            # Compute three contrasts #\n",
    "            sub_conts_a[win] = fmri_glm.compute_contrast(\n",
    "                    win, output_type='effect_size') #effect_size\n",
    "            # each window saved as a key\n",
    "            group_sub_glm_a[sub] = sub_conts_a\n",
    "            ## Save ## \n",
    "            np.save(os.path.join(out_dir+sav_dir, '%s_%s.npy') %(cond_a, prefix), group_sub_glm_a)\n",
    "\n",
    "            ## COND B ## \n",
    "            fmri_glm = fmri_glm.fit(cond_b_runs, design_matrices=design_matrices_b) \n",
    "            # Compute three contrasts #\n",
    "            sub_conts_b[win] = fmri_glm.compute_contrast(\n",
    "                    win, output_type='effect_size') #effect_size\n",
    "\n",
    "            group_sub_glm_b[sub] = sub_conts_b\n",
    "            ## Save ## \n",
    "            np.save(os.path.join(out_dir+sav_dir, '%s_%s.npy') %(cond_b,prefix), group_sub_glm_b)\n",
    "\n",
    "\n",
    "            # subtract images\n",
    "            full_contrast_win = math_img(\"img1 - img2\",img1=group_sub_glm_a[sub][win],img2=group_sub_glm_b[sub][win])\n",
    "\n",
    "            # save individual full contrast subtracted images for t-test\n",
    "            nib.save(full_contrast_win, os.path.join(\n",
    "                out_dir+sav_dir,'%s_%s_%s_%s_zmap.nii') % (sub,cond_a+'-'+cond_b, prefix, win))\n",
    "            \n",
    "            print('finish',win)\n",
    "        print('finish', sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-majority",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
